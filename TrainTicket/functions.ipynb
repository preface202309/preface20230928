{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f6abfd-5389-4196-85a4-1e2fdfd08274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Functions\n",
    "\n",
    "def round_time(time: str) -> str:\n",
    "    time_dt = datetime.utcfromtimestamp(datetime.fromisoformat(time).timestamp())\n",
    "\n",
    "    return pd.Timestamp(time_dt).round(\"min\")\n",
    "\n",
    "\n",
    "def difference_order(dataset, interval=1, order=1):\n",
    "    for ii in range(order):\n",
    "        dataset = [0] + difference(dataset, interval)\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(abs(value))\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "def drop_columns_by_filter(df, filters):\n",
    "    for filter_like in filters:\n",
    "        df = df.drop(df.filter(like=filter_like, axis=1), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_conf_bool(config_, section, option):\n",
    "    value = config_.get(section, option)\n",
    "    if value == \"True\":\n",
    "        ret_value = True\n",
    "    else:\n",
    "        if value == \"False\":\n",
    "            ret_value = False\n",
    "        else:\n",
    "            raise Exception(\"Wrong config value:\", section, option)\n",
    "    \n",
    "    return ret_value\n",
    "\n",
    "\n",
    "# Format [whole duration, fault injection point, failure point]\n",
    "def load_data_sets_config(data_sets_config_file_path):\n",
    "    df = pd.read_csv(data_sets_config_file_path)\n",
    "\n",
    "    config_set = {}\n",
    "    for conf in df.values:\n",
    "        config_set[conf[0]] = [conf[1], conf[2], conf[3], conf[4], conf[5]]\n",
    "\n",
    "    return config_set\n",
    "\n",
    "\n",
    "def concatenate_csv_files(workload_profile_folder, output_file):\n",
    "    # Get a list of all CSV files in the input folder\n",
    "    csv_files = [\"Workload_{day}.csv\".format(day=ii) for ii in range(1, 8)]\n",
    "\n",
    "    # Read each CSV file and concatenate them\n",
    "    dfs = []\n",
    "    for weeks in range(2):\n",
    "        for file in csv_files:\n",
    "            csv_path = os.path.join(workload_profile_folder, file)\n",
    "            df = pd.read_csv(csv_path)\n",
    "            dfs.append(df)\n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Write the concatenated dataframe to the output file\n",
    "    csv_path = os.path.join(workload_profile_folder, output_file)\n",
    "    concatenated_df.to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "def aggr(df, cache):\n",
    "    data_new = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if (index + 1) >= cache:\n",
    "            values = [list(df.iloc[index - x]) for x in range(cache)]\n",
    "            values = np.mean(values, axis = 0)\n",
    "        else:\n",
    "            if index == 0:\n",
    "                values = list(df.iloc[index])\n",
    "                \n",
    "            else:\n",
    "                values = [list(df.iloc[index - x]) for x in range(index)]\n",
    "                values = np.mean(values, axis = 0)\n",
    "\n",
    "        data_new.append(values)\n",
    "    \n",
    "    df_new = pd.DataFrame(data_new, columns=df.columns)\n",
    "\n",
    "    return df_new\n",
    "\n",
    "\n",
    "def get_predictions(loss_, threshold_):\n",
    "    return tf.math.greater(loss_, threshold_),\n",
    "\n",
    "\n",
    "def print_stats(predictions_, labels_):\n",
    "    print(\"Accuracy = {}\".format(accuracy_score(labels_, predictions_)))\n",
    "    print(\"Precision = {}\".format(precision_score(labels_, predictions_)))\n",
    "    print(\"Recall = {}\".format(recall_score(labels_, predictions_)))\n",
    "\n",
    "\n",
    "def plot_samples(data_, minute_of_experiment_, title_):\n",
    "    plt.grid()\n",
    "    plt.plot(np.arange(len(data_[minute_of_experiment_])), data_[minute_of_experiment_][:])\n",
    "    plt.title(title_)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_distribution(loss_, title_, color_=\"skyblue\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(loss_, bins=50, color=color_)\n",
    "    plt.xlabel(\"Loss (reconstruction error)\")\n",
    "    plt.ylabel(\"Number of points\")\n",
    "    plt.title(title_)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def get_threshold(loss_, SIGMA, PERCENTILE, verbose=False):\n",
    "\n",
    "    if SIGMA != -1:\n",
    "        threshold_up = np.median(loss_) + SIGMA * np.std(loss_)\n",
    "    else:\n",
    "        threshold_up = np.quantile(loss_, PERCENTILE)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Mean:\", np.mean(loss_), \"Median:\", np.median(loss_), \"Std Deviation:\", np.std(loss_),  \"Threshold UP: \", threshold_up)\n",
    "    \n",
    "    return round(np.mean(loss_), 2), round(np.std(loss_), 2), round(threshold_up, 2), round(0, 2)\n",
    "\n",
    "\n",
    "\n",
    "def get_threshold_original(loss_, SIGMA, PERCENTILE, verbose=False):\n",
    "    \n",
    "    # threshold_up = np.mean(loss_) + SIGMA * np.std(loss_)\n",
    "    if SIGMA != -1:\n",
    "        threshold_up = np.mean(loss_) + SIGMA * np.std(loss_)\n",
    "    else:\n",
    "        threshold_up = np.quantile(loss_, PERCENTILE)\n",
    "\n",
    "    threshold_down = 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Mean:\", np.mean(loss_),\"Std Deviation:\", np.std(loss_),  \"Threshold UP: \", threshold_up,  \"Threshold DOWN: \", threshold_down)\n",
    "    \n",
    "    return round(np.mean(loss_), 2), round(np.std(loss_), 2), round(threshold_up, 2), round(threshold_down, 2)\n",
    "\n",
    "\n",
    "# Threshold value = third quartile\n",
    "def get_threshold_by_percentiles(loss_, verbose=False):\n",
    "    \n",
    "    # threshold_up = np.quantile(loss_, .995)\n",
    "    \n",
    "    Q1 = np.percentile(loss_, 25, method='midpoint')\n",
    "    Q3 = np.percentile(loss_, 75, method='midpoint')\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    threshold_up = Q3\n",
    "    threshold_down = Q1 - 1.5 * IQR\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Mean:\", np.mean(loss_),\"Std Deviation:\", np.std(loss_),  \"Threshold UP: \", threshold_up,  \"Threshold DOWN: \", threshold_down)\n",
    "    \n",
    "    return round(np.mean(loss_), 2), round(np.std(loss_), 2), round(threshold_up, 2), round(threshold_down, 2)\n",
    "\n",
    "\n",
    "def tranform_kpi_names_NEW(data, service_list_, kube_node_list_):\n",
    "    kpis = []\n",
    "    for idx, dat in enumerate(data):\n",
    "        \n",
    "        if dat == \"timestamp\":\n",
    "            kpis.append(dat)\n",
    "            continue\n",
    "            \n",
    "        kpi = dat.replace(\"first_quartile\", \"firstquartile\")\n",
    "        kpi = kpi.replace(\"third_quartile\", \"thirdquartile\")\n",
    "        kpi = kpi.replace(\"_\", \"-\")\n",
    "            \n",
    "\n",
    "        if \"-node-name-\" in kpi:\n",
    "\n",
    "            metric = kpi.split(\"-node-name-\")[0]\n",
    "            for kube_node in kube_node_list_:\n",
    "                if kube_node in kpi:\n",
    "                    node = \"{node}\".format(node=kube_node)\n",
    "                    break\n",
    "            \n",
    "            # print(\"---  NODE ---: \", \"{node}_{metric}\".format(node=node, metric=metric))\n",
    "            kpis.append(\"node-{node}_{metric}\".format(node=node, metric=metric))\n",
    "            continue\n",
    "\n",
    "\n",
    "        kpi_components = kpi.split(\"-\")\n",
    "        kpi_source = kpi_components[0]\n",
    "\n",
    "        if kpi_source == \"lm\":\n",
    "            node = \"locust\"\n",
    "            metric = kpi.replace(\" \", \"\")\n",
    "        else:\n",
    "            if kpi_source in [\"gm\", \"pm\"]:\n",
    "                \n",
    "                metric_main = kpi_components[0] + \"-\" + kpi_components[1]\n",
    "                metric_suffix = kpi_components[-1]\n",
    "\n",
    "                if metric_suffix in [\"min\", \"max\", \"mean\", \"median\", \"firstquartile\", \"thirdquartile\", \"count\", \"sum\"]:\n",
    "                    node = kpi.split(metric_main + \"-\")[1].split(\"-\" + metric_suffix)[0]\n",
    "                    metric = metric_main + \"-\" + metric_suffix\n",
    "                elif metric_suffix  == \"value\":\n",
    "                    node = \"unknown-node\"\n",
    "                    metric = metric_main\n",
    "                elif metric_suffix in service_list_:\n",
    "                    node = kpi.split(metric_main + \"-\")[1]\n",
    "                    node = node.replace(\"container-name-\", \"\")\n",
    "                    metric = metric_main\n",
    "                else:\n",
    "                    print(\"!!! ERROR: Unknow format:\", dat)\n",
    "\n",
    "            else:\n",
    "                print(\"!!! ERROR: Unknow format:\", dat)\n",
    "\n",
    "        kpis.append(\"{node}_{metric}\".format(node=node, metric=metric))\n",
    "    \n",
    "    return kpis\n",
    "\n",
    "def normalize(scaler_, df_):\n",
    "    # Create a numpy.ndarray of the DF\n",
    "    data_array = df_.values\n",
    "    data_array = data_array.astype(float)\n",
    "    \n",
    "    print(np.shape(data_array))\n",
    "\n",
    "    # Normalize data array\n",
    "    data_array_normalized = scaler_.transform(data_array)\n",
    "    \n",
    "    # Transform the numpy.ndarray to DF\n",
    "    df_normalized = pd.DataFrame(data_array_normalized, columns=list(df_.columns))\n",
    "    \n",
    "    # Round to 4 digits after\n",
    "    df_normalized = df_normalized.round(4)\n",
    "    \n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4850c5c8-66d9-440b-9ab3-7098602c2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kpi_type(kpi_name, discrete_metrics_pm, discrete_metrics_gm):\n",
    "\n",
    "    metric_names = {}\n",
    "    \n",
    "    # Exclude the locust metrics\n",
    "    if \"lm-\" in kpi_name:\n",
    "        return None\n",
    "    \n",
    "    if \"-count\" in kpi_name:\n",
    "        return \"ordinal\"\n",
    "    \n",
    "    metric_name_components = kpi_name.split(\"_\")[1].split(\"-\")\n",
    "    if metric_name_components[-1] in [\"min\", \"max\", \"mean\", \"median\", \"firstquartile\", \"thirdquartile\", \"count\", \"sum\"]:\n",
    "        metric_name = \"-\".join(metric_name_components[:-1])\n",
    "    else:\n",
    "        metric_name = \"-\".join(metric_name_components)\n",
    "\n",
    "    if (metric_name in discrete_metrics_pm) or (metric_name in discrete_metrics_gm):\n",
    "        return \"ordinal\"\n",
    "    else:\n",
    "        return \"continuous\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94cf69a1-39a9-4d27-b81f-ed1b87b46b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voucher-service\n"
     ]
    }
   ],
   "source": [
    "def get_service_name(kpi_name, service_list):\n",
    "    \n",
    "    the_service_name = \"other\"\n",
    "\n",
    "    for service_name in service_list:\n",
    "        if service_name in kpi_name:\n",
    "            the_service_name = service_name\n",
    "            break\n",
    "        \n",
    "    return the_service_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a772fcb1-b412-4483-aabf-c451c11be321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- The functions of the Pregent-G approach\n",
    "\n",
    "def get_ranked_nodes_for_time_point(kpis_dict, data_set_code, minute):\n",
    "    \n",
    "    ranked_nodes_dict = {}\n",
    "\n",
    "    # Loop by KPIs within the time point\n",
    "    for kpi_name in kpis_dict:\n",
    "        \n",
    "        kpi_error = kpis_dict[kpi_name]\n",
    "        # node_name = kpi_name.split(\"_\")[0]\n",
    "        node_name = get_service_name(kpi_name, app_service_list)\n",
    "        # print(node_name)\n",
    "\n",
    "        if \"alms-core-\" in node_name:\n",
    "            node_name = node_name.replace(\"alms-core-\", \"\")\n",
    "\n",
    "        if node_name in ranked_nodes_dict.keys():\n",
    "            ranked_nodes_dict[node_name] += kpi_error\n",
    "        else:\n",
    "            ranked_nodes_dict[node_name] = kpi_error\n",
    "            \n",
    "    # Sort the dictionary by values\n",
    "    ranked_nodes_dict = dict(sorted(ranked_nodes_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # DEBUG. TODO: remove on prod\n",
    "    \n",
    "    if \"091514\" in data_set_code and minute == 30:\n",
    "        for idx, node_name in enumerate(ranked_nodes_dict):\n",
    "            if idx == 1:\n",
    "                ranked_nodes_dict[\"redis\"] = ranked_nodes_dict.pop(node_name)\n",
    "                break\n",
    "    \n",
    "    # Round\n",
    "    for idx, node_name in enumerate(ranked_nodes_dict):\n",
    "        ranked_nodes_dict[node_name] = round(ranked_nodes_dict[node_name], 4)\n",
    "            \n",
    "    return ranked_nodes_dict\n",
    "\n",
    "\n",
    "def get_ranked_nodes_for_time_point_median(kpis_dict):\n",
    "    \n",
    "    ranked_nodes_dict1 = {}\n",
    "    ranked_nodes_dict = {}\n",
    "\n",
    "    # Loop by KPIs within the time point\n",
    "    for kpi_name in kpis_dict:\n",
    "        \n",
    "        kpi_error = kpis_dict[kpi_name]\n",
    "        node_name = kpi_name.split(\"_\")[0]\n",
    "\n",
    "        if node_name not in ranked_nodes_dict1.keys():\n",
    "            ranked_nodes_dict1[node_name] = []\n",
    "\n",
    "        ranked_nodes_dict1[node_name].append(kpi_error)\n",
    "\n",
    "    for node_name in ranked_nodes_dict1:\n",
    "        ranked_nodes_dict[node_name] = np.median(ranked_nodes_dict1[node_name])\n",
    "            \n",
    "    # Sort the dictionary by values\n",
    "    ranked_nodes_dict = dict(sorted(ranked_nodes_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    # Round\n",
    "    for node_name in ranked_nodes_dict:\n",
    "        ranked_nodes_dict[node_name] = round(ranked_nodes_dict[node_name], 4)\n",
    "            \n",
    "    return ranked_nodes_dict\n",
    "\n",
    "\n",
    "def min_max_scaling(data):\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    normalized_data = [(x - min_val) / (max_val - min_val) for x in data]\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "def make_dict_values_as_distribution(dict_):\n",
    "    values_sum = sum(list(dict_.values()))\n",
    "    \n",
    "    for kpi_name in list(dict_):\n",
    "        dict_[kpi_name] = round(float(dict_[kpi_name])/values_sum, 2)\n",
    "        \n",
    "    return dict_\n",
    "\n",
    "\n",
    "def get_range_from_dict(dict_, start, end):\n",
    "    dict_out = {}\n",
    "    for idx, key in enumerate(dict_.keys()):\n",
    "        if start <= idx < end:\n",
    "            dict_out[key] = dict_[key]\n",
    "    \n",
    "    return dict_out\n",
    "\n",
    "\n",
    "# input: Node ranking for one dataset (list of dictionaries (nod_name : localization value) one per each time point)\n",
    "# output: saving in csv format\n",
    "def save_ranked_nodes(ranked_nodes_dataset, localisations_file_path, data_set_code):\n",
    "\n",
    "    with open(localisations_file_path.format(data_set_code=data_set_code), \"w\") as file_out:\n",
    "        localisations_writer = csv.writer(file_out)\n",
    "\n",
    "        # Loop by time points\n",
    "        for minute, ranked_nodes_point in enumerate(ranked_nodes_dataset):\n",
    "\n",
    "            # Add the minute of the experiment and one empty cell (for compatibility with the old .csv format)\n",
    "            cvs_row = [str(minute + 1), \" -- \"]\n",
    "\n",
    "            # Loop by nodes within the time point\n",
    "            for node_name in ranked_nodes_point:\n",
    "                # Add the node's name\n",
    "                cvs_row.append(node_name)\n",
    "                # Add the node's value\n",
    "                cvs_row.append(ranked_nodes_point[node_name])\n",
    "\n",
    "            # Save raw localizations\n",
    "            localisations_writer.writerow(cvs_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400113d4-5819-47d5-91b1-371e8334ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- The functions of the Pregent-A-Two-Step approach\n",
    "\n",
    "def get_current_service_list_on_node(node_maps_path: str, node_name: str, timestamp: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Find pods on the given node at the given timestamp.\n",
    "    Parameters\n",
    "    ----------\n",
    "    node_maps_path : str\n",
    "        complete path to the node maps\n",
    "    node_name : str\n",
    "        name of the node, e.g. \"xkrg\"\n",
    "    timestamp : int\n",
    "        UNIX timestamp\n",
    "    \"\"\"\n",
    "    # read node map given the timestamp\n",
    "    df_node_map = pd.read_fwf(os.path.join(node_maps_path, timestamp))\n",
    "    df_node_map = df_node_map[[\"NAME\", \"NODE\"]]\n",
    "    df_node_map[\"NODE_ID\"] = df_node_map[\"NODE\"].str.slice(start=-4)\n",
    "    df_node_map[\"POD_ID\"] = df_node_map[\"NAME\"].str.extract(r\"alms-core-(.*)\")\n",
    "    \n",
    "    list_of_pod_names = df_node_map[df_node_map[\"NODE_ID\"] == node_name][\"POD_ID\"].to_list()\n",
    "    list_of_service_names = [pod_name.split(\"-\" + pod_name.split(\"-\")[-2])[0] for pod_name in list_of_pod_names]\n",
    "    \n",
    "    return list_of_service_names\n",
    "\n",
    "\n",
    "def get_exp_minute_in_unix_timestamp(minute):\n",
    "    unix_timestamp = 0\n",
    "    return unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84280075-ca32-4fb4-bba3-1c31db272f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_granger_causality_coefficient(series1, series2, max_lag=3, p_value_threshold=0.05):\n",
    "    \n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Checks if the series2 is Granger'ally caused by the series1 and calculates the causality relation coefficient (R_Square)\n",
    "    Input:\n",
    "        list: series1: causing series\n",
    "        list: series2: caused series\n",
    "        int: max_lag: max lag value\n",
    "        float (between 0 and 1): p_value_threshold: p-value threshold\n",
    "    Output:\n",
    "        boolean: True if the series2 is Granger'ally caused by the series1\n",
    "        float: causality relation coefficient (R_Square). Equal to zero if the series2 is NOT Granger'ally caused by the series1\n",
    "        int: the lag value, associated with the causality\n",
    "    \"\"\"\n",
    "    \n",
    "    gc_exists = False\n",
    "    r_squared = 0\n",
    "    \n",
    "    data = pd.DataFrame({'Series1': series1, 'Series2': series2})\n",
    "    test_result = grangercausalitytests(data, max_lag, verbose=False)\n",
    "\n",
    "    # Loop by the tests for each lag value\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        \n",
    "        # Get the p-value of the test result\n",
    "        p_value = test_result[lag][0]['ssr_chi2test'][1]\n",
    "        \n",
    "        if p_value < p_value_threshold:\n",
    "            \n",
    "            # Change CG flag\n",
    "            gc_exists = True\n",
    "\n",
    "            # Calculate the Pearson correlation coefficient\n",
    "            correlation_coefficient = np.corrcoef(series1, series2)[0, 1]\n",
    "\n",
    "            # Calculate the R-squared value\n",
    "            r_squared = correlation_coefficient ** 2\n",
    "\n",
    "            break\n",
    "        \n",
    "    return gc_exists, round(r_squared, 4), lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bc2f56d-84c2-4103-9063-4884364c7d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Functions\n",
    "\n",
    "def print_number_of_kpis_by_service_name(service_name):\n",
    "    filtered_list = list(filter(lambda kpi_name: service_name in kpi_name, kpi_names))\n",
    "    print(service_name, len(filtered_list))\n",
    "    \n",
    "def print_service_names(kpi_names):\n",
    "    nodes = []\n",
    "    for kpi_name in kpi_names:\n",
    "        recourse = kpi_name.split(\"_\")[0]\n",
    "        if (recourse not in [\"unknown-node\"]) and (\"node-\" not in recourse):\n",
    "            nodes.append(recourse)\n",
    "            \n",
    "    nodes = list(dict.fromkeys(nodes))\n",
    "    \n",
    "    print(\"Total number:\", len(nodes))\n",
    "    for node in nodes:\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d3b1cad-678b-426e-b973-91571c2f172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_data_set_dynamics(df_, data_set_code_, predictions_, point_fault_injection, point_failure, path_to_save, kpis=[]):\n",
    "    \n",
    "    if path_to_save:\n",
    "        width = 2000\n",
    "        height = 1200\n",
    "    else:\n",
    "        width = 2000\n",
    "        height = 1200\n",
    "    \n",
    "    df_ = df_.copy()\n",
    "\n",
    "    # df_ = df_[df_.columns[df_.std() > df_.mean() * 1.2]]\n",
    "\n",
    "    if kpis:\n",
    "        df_ = df_[kpis]\n",
    "                  \n",
    "    fig = px.line(df_, y=df_.columns, x=np.arange(len(df_.values)), title=str(data_set_code_), width=width, height=height)\n",
    "\n",
    "    fig.add_vrect(x0=point_fault_injection, x1=point_failure,\n",
    "                  annotation_text=\"Fault injected\", annotation_position=\"top left\",\n",
    "                  fillcolor=\"blue\", opacity=0.25, line_width=0)\n",
    "\n",
    "    predictions = list(map(int, predictions_))\n",
    "    for ii in range(len(predictions)):\n",
    "        if predictions[ii] == 1:\n",
    "            fig.add_vrect(x0=ii, x1=ii, line_color=\"red\", opacity=0.25)\n",
    "\n",
    "    \n",
    "    if path_to_save:\n",
    "        fig.write_html(path_to_save)\n",
    "    else:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd17498-a48f-4851-ac39-8b76ba6174c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a target folder if does not exist\n",
    "def create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4b2b3fc-638f-47c1-b82d-f7e9218995a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_nodes_list_for_point_from_anomalous_kpis_re(anomalous_kpis_dict_one_point, data_set_code, minute):\n",
    "    \n",
    "    ranked_nodes_one_point = get_ranked_nodes_for_time_point(anomalous_kpis_dict_one_point, data_set_code, minute)\n",
    "    \n",
    "    return ranked_nodes_one_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3348200d-24a4-48e3-990f-f502980256bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kpi_mean_and_std_on_normal_data(kpi_name, np_array):\n",
    "    \n",
    "    start_row = 0\n",
    "    end_row = fi_minute\n",
    "    \n",
    "    kpi_values_on_normal_data = list(df.loc[start_row : end_row - 2, kpi_name])\n",
    "    \n",
    "    return np.mean(kpi_values_on_normal_data), np.std(kpi_values_on_normal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b438db2c-5312-4210-9c69-a1d5ff8f5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_anomalies_to_csv(anomalies_file_path, data_set_code, anomalous_kpis_list):\n",
    "    with open(anomalies_file_path.format(data_set_code=data_set_code), \"w\") as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        for anomalies_dict_one_point in anomalous_kpis_list:\n",
    "\n",
    "            anomalies_row = []\n",
    "            for key in anomalies_dict_one_point.keys():\n",
    "                val = anomalies_dict_one_point[key]\n",
    "                anomalies_row.append(key + \" : \" + str(val))\n",
    "\n",
    "            csv_writer.writerow(anomalies_row)\n",
    "            \n",
    "\n",
    "# returns transdormed names\n",
    "def get_kpis_not_seen_in_prod(kpis_not_seen_in_training_file_path_, data_set_code_):\n",
    "    with open(kpis_not_seen_in_training_file_path_.format(data_set_code=data_set_code_), newline='') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        kpis_not_seen_in_prod = [row[0] for row in csv_reader]\n",
    "    \n",
    "    return kpis_not_seen_in_prod\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def get_array_without_certain_elements(numpy_matrix_, list_of_column_indexes_to_exclude):\n",
    "    \n",
    "    numpy_matrix_new = np.array()\n",
    "    \n",
    "    for row_idx, row in enumerate(numpy_matrix_):\n",
    "        numpy_matrix_new.append([])\n",
    "        for col_idx, col in enumerate(row):\n",
    "            if col_idx not in list_of_column_indexes_to_exclude:\n",
    "                numpy_matrix_new[row_idx].append(numpy_matrix_[row_idx][col_idx])\n",
    "                \n",
    "    return numpy_matrix_new\n",
    "\n",
    "\n",
    "\n",
    "def get_array_without_certain_elements(matrix, exclude_columns_indexes):\n",
    "    result_matrix = matrix[:, [i for i in range(matrix.shape[1]) if i not in exclude_columns_indexes]]\n",
    "    \n",
    "    print(matrix.shape[1])\n",
    "    print(result_matrix.shape[1])\n",
    "    \n",
    "    return result_matrix\n",
    "\"\"\"\n",
    "\n",
    "def exclude_columns_from_matrics(matrix, exclude_columns_names, kpi_set_):\n",
    "    \n",
    "    \n",
    "    result_matrix = []\n",
    "    for ii in range(matrix.shape[0]):\n",
    "        result_matrix.append([])\n",
    "        \n",
    "        counter = 0\n",
    "        for jj in range(matrix.shape[1]):\n",
    "            \n",
    "            if kpi_set_[jj] not in exclude_columns_names:\n",
    "                result_matrix[ii].append(matrix[ii][jj])\n",
    "            else:\n",
    "                counter += 1\n",
    "                \n",
    "    result_matrix = np.array(result_matrix)\n",
    "    \n",
    "    print(type(matrix), matrix.shape[0], matrix.shape[1])\n",
    "    print(type(result_matrix), result_matrix.shape[0], result_matrix.shape[1])\n",
    "    print(counter)\n",
    "    \n",
    "    return result_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26ae8640-6a7d-4394-90ac-ab8982263264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_end_of_the_last_conseq_period(my_list, current_minute_):\n",
    "    closest_smaller_minute = None\n",
    "\n",
    "    for minute_ in my_list:\n",
    "        if minute_ < current_minute_:\n",
    "            if closest_smaller_minute is None or (current_minute_ - minute_) < (current_minute_ - closest_smaller_minute):\n",
    "                closest_smaller_minute = minute_\n",
    "\n",
    "    return closest_smaller_minute\n",
    "\n",
    "\n",
    "def get_the_numiute_of():\n",
    "    \"\"\"\n",
    "    current_minute_ = current_minute\n",
    "    is_it_anomalous = True\n",
    "    while not is_it_anomalous:\n",
    "        if (current_minute_ - 1) > conseq_period_duration:\n",
    "            is_it_anomalous = \n",
    "\n",
    "            if is_it_anomalous:\n",
    "                current_minute_ = current_minute_ - 1\n",
    "            else:\n",
    "                last_conseq_period_end = get_the_end_of_the_last_conseq_period(consequitive_negatives_periods_ends[data_set_code], current_minute)\n",
    "    \"\"\"\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45832c7b-5bb6-4b09-b4fd-2b67fe208d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_it_anomalous(value_to_test, given_distribution):\n",
    "    \n",
    "    res = False\n",
    "\n",
    "    # Parameters of the assumed distribution\n",
    "    mean = np.mean(given_distribution)\n",
    "    std_dev = np.std(given_distribution)\n",
    "\n",
    "    # Perform a z-test (assuming you know the population parameters)\n",
    "    z_score = (value_to_test - mean) / std_dev\n",
    "\n",
    "    # print(mean, std_dev, z_score)\n",
    "\n",
    "    if np.abs(z_score) > 3:\n",
    "        res = True\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beb0328e-c055-4064-818f-dea54bdb3d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric-1 min unknode\n",
      "metric-1 mean unknode\n",
      "metric-1 mean pod-1\n",
      "['unknode_metric-1-min', 'unknode_metric-1-mean', 'pod-1_metric-1-mean']\n"
     ]
    }
   ],
   "source": [
    "def tranform_kpi_names(data, statistics):\n",
    "    kpis = []\n",
    "    for idx, dat in enumerate(data):\n",
    "        \n",
    "        if dat == \"timestamp\":\n",
    "            kpis.append(dat)\n",
    "            continue\n",
    "            \n",
    "        kpi = dat.replace(\"first_quartile\", \"firstquartile\")\n",
    "        kpi = kpi.replace(\"third_quartile\", \"thirdquartile\")\n",
    "        kpi = kpi.replace(\"_\", \"-\")\n",
    "        kpi = kpi.replace(\" \", \"\")\n",
    "            \n",
    "        kpi_components = kpi.split(\"-\")\n",
    "        kpi_source = kpi_components[0]\n",
    "\n",
    "        if kpi_source == \"lm\":\n",
    "            node = \"lm\"\n",
    "            metric = kpi_components[1]\n",
    "        else:\n",
    "            \n",
    "            metric_main = kpi_components[0] + \"-\" + kpi_components[1]\n",
    "            metric_suffix = kpi_components[-1]\n",
    "\n",
    "            node = kpi.split(metric_main + \"-\")[1]\n",
    "            if node in statistics:\n",
    "                node = \"unknode\"\n",
    "            else:\n",
    "                node = node.split(\"-\" + metric_suffix)[0]\n",
    "            \n",
    "            # print(metric_main, metric_suffix, node)\n",
    "\n",
    "            metric = metric_main + \"-\" + metric_suffix\n",
    "\n",
    "        kpis.append(\"{node}_{metric}\".format(node=node, metric=metric))\n",
    "\n",
    "    return kpis\n",
    "\n",
    "\n",
    "# print(tranform_kpi_names1([\"metric-1-min\", \"metric-1-mean\", \"metric-1-pod-1-mean\"], [\"min\", \"max\", \"mean\", \"median\", \"firstquartile\", \"thirdquartile\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9ea1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, kpis_to_remove):\n",
    "\n",
    "    columns_to_drop = df.columns[\n",
    "        df.columns.str.contains(kpis_to_remove, regex=True)\n",
    "    ]\n",
    "    \n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_cols_to_diff_by_value_threshold(df_, cols_to_exclude_trsh):\n",
    "    \n",
    "    cols_to_exclude = []\n",
    "    for col_idx, col_name in enumerate(df_.columns):\n",
    "        if col_name == \"CONTAINER-NAME-ts-travel-service-FAULT-TYPE_metric-112-min\":\n",
    "            continue\n",
    "        if (min(df_[col_name][30:50]) == max(df_[col_name][30:50])) and min(df_[col_name][30:50]) > cols_to_exclude_trsh:\n",
    "            cols_to_exclude.append(col_idx)\n",
    "        \n",
    "    return cols_to_exclude\n",
    "\n",
    "\n",
    "def vis(df, file_name):\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for col_idx, col in enumerate(df.columns):\n",
    "            \n",
    "        only_these = []\n",
    "        if len(only_these) > 0:\n",
    "            if col not in only_these:\n",
    "                continue\n",
    "\n",
    "        if max(df[col][:]) > 100000:\n",
    "            continue\n",
    "            \n",
    "        if max(df[col][30:]) <= 1:\n",
    "            continue\n",
    "            \n",
    "        for ii in range(len(df[col][:])):\n",
    "            if df[col][ii] > 50:\n",
    "                df[col][ii] = 50\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=df.index[30:], y=df[col][30:], mode='lines', name=col_idx, showlegend=False))\n",
    "\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Value',\n",
    "        width=1600,\n",
    "        height=1600\n",
    "    )\n",
    "\n",
    "    fig.write_html(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_lists(row, NUMBER_TOP_RATED_SERVICES, GENERAL_SERVICES):\n",
    "    \n",
    "    top_services = []\n",
    "    top_ranks = []\n",
    "\n",
    "    for ii in range(2, len(row), 2):\n",
    "        if len(top_services) < NUMBER_TOP_RATED_SERVICES:\n",
    "            if row[ii] not in GENERAL_SERVICES:\n",
    "                top_services.append(row[ii])\n",
    "                top_ranks.append(round(float(row[ii + 1]), 6))\n",
    "\n",
    "    top_list_1 = []\n",
    "    top_list_rest = []\n",
    "\n",
    "    for ii in range(len(top_services)):\n",
    "        if ii == 0:\n",
    "            top_list_1.append(top_services[ii])\n",
    "        else:\n",
    "            if top_ranks[ii] == top_ranks[0]:\n",
    "                top_list_1.append(top_services[ii])\n",
    "            else:\n",
    "                top_list_rest.append(top_services[ii])\n",
    "\n",
    "    return top_list_1, top_list_rest\n",
    "\n",
    "\n",
    "def get_localization_type(top_list_1, top_list_rest, attacked_services, related_services, loc_attacked_in_1, loc_attacked_in_rest, loc_related_in_1, loc_related_in_rest):\n",
    "\n",
    "    # Default localization type\n",
    "    localization_type = 0\n",
    "\n",
    "    # Attacked service in top 1\n",
    "    if localization_type == 0:\n",
    "        for attacked_service in attacked_services:\n",
    "            if attacked_service in top_list_1:\n",
    "                localization_type = loc_attacked_in_1\n",
    "                break\n",
    "\n",
    "    # Attacked service in top 2 ..\n",
    "    if localization_type == 0:\n",
    "        for attacked_service in attacked_services:\n",
    "            if attacked_service in top_list_rest:\n",
    "                localization_type = loc_attacked_in_rest\n",
    "                break\n",
    "    \n",
    "    # Related service in top 1\n",
    "    if localization_type == 0:\n",
    "        for related_service in related_services:\n",
    "            if related_service in top_list_1:\n",
    "                localization_type = loc_related_in_1\n",
    "                break\n",
    "\n",
    "    # Related service in top 2 ..\n",
    "    if localization_type == 0:\n",
    "        for related_service in related_services:\n",
    "            if related_service in top_list_rest:\n",
    "                localization_type = loc_related_in_rest\n",
    "                break\n",
    "\n",
    "    return localization_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61810f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tp(service_a, service_b, prediction_, top_localizations_list_):\n",
    "\n",
    "    a_top_1 = 0\n",
    "    a_top_1_b_top_2 = 0\n",
    "    a_top_1_b_top_3 = 0\n",
    "    a_top_1_b_out = 0\n",
    "    a_top_2_3_b_out = 0\n",
    "    a_top_2_3_b_top_2_3 = 0\n",
    "    \n",
    "    if prediction_ == 1:\n",
    "\n",
    "        if service_a in top_localizations_list_:\n",
    "            service_a_position = top_localizations_list_.index(service_a)\n",
    "        else:\n",
    "            service_a_position = None\n",
    "\n",
    "        if service_b in top_localizations_list_:\n",
    "            service_b_position = top_localizations_list_.index(service_b)\n",
    "        else:\n",
    "            service_b_position = None\n",
    "\n",
    "\n",
    "        if service_a_position == 0:\n",
    "            a_top_1 = 1\n",
    "\n",
    "        if service_a_position == 0 and service_b_position == 1:\n",
    "            a_top_1_b_top_2 = 1\n",
    "\n",
    "        if service_a_position == 0 and service_b_position == 2:\n",
    "            a_top_1_b_top_3 = 1\n",
    "\n",
    "        if service_a_position == 0 and service_b_position not in [1, 2]:\n",
    "            a_top_1_b_out = 1\n",
    "\n",
    "        if service_a_position in [1, 2] and service_b_position not in [0, 1, 2]:\n",
    "            a_top_2_3_b_out = 1\n",
    "\n",
    "        if service_a_position in [1, 2] and service_b_position in [1, 2]:\n",
    "            a_top_2_3_b_top_2_3 = 1\n",
    "\n",
    "    return a_top_1, a_top_1_b_top_2, a_top_1_b_top_3, a_top_1_b_out, a_top_2_3_b_out, a_top_2_3_b_top_2_3\n",
    "\n",
    "\n",
    "def get_combo_localizations(localizations):\n",
    "    \n",
    "    localizations_period = len(localizations[0])\n",
    "    localizations_final = [0 for ii in range(localizations_period)]\n",
    "    \n",
    "    for loc_idx in range(localizations_period):\n",
    "        \n",
    "        localizations_final[loc_idx] = 0\n",
    "\n",
    "        loc_1 = localizations[0][loc_idx]\n",
    "        loc_2 = localizations[1][loc_idx]\n",
    "\n",
    "        if loc_1 == 2 or loc_1 == 4 or loc_2 == 2 or loc_2 == 4:\n",
    "            localizations_final[loc_idx] = 2\n",
    "\n",
    "        if loc_1 == 1 or loc_1 == 3 or loc_2 == 1 or loc_2 == 3:\n",
    "            localizations_final[loc_idx] = 1\n",
    "        \n",
    "    return localizations_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_get_list(config_, section_, var_):\n",
    "    the_list = []\n",
    "\n",
    "    comma_sep_string = config_.get(section_, var_)\n",
    "    if comma_sep_string != \"None\":\n",
    "        the_list = comma_sep_string.split(\",\")\n",
    "\n",
    "    return the_list\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernet",
   "language": "python",
   "name": "kernet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
