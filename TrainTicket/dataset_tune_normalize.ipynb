{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6462e46-d2f2-4014-b6ae-e4e97e98a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from warnings import simplefilter\n",
    "import matplotlib.pyplot as plt\n",
    "import configparser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0479df45-d519-4c99-925b-ffef9402011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Functions\n",
    "\n",
    "%run functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c0a3302-babe-4f78-9cf2-ae36f1f1c673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file loaded: config-train_ticket.ini\n"
     ]
    }
   ],
   "source": [
    "# --- Init Configuration Parameters\n",
    "\n",
    "%run predict_notebook_sections/configuration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b9f112-4da3-43ac-a753-db9ea95f4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load training data\n",
    "\n",
    "train_data_df = pd.read_csv(consolidated_data_set_file_path.format(data_set_code=train_data_set_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b237577-d6b9-4264-980d-8cd7301e739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set shape: (19996, 12525)\n"
     ]
    }
   ],
   "source": [
    "# --- Format the training data set\n",
    "\n",
    "# Add the timestamp column to the training df\n",
    "train_data_df['timestamp'] = [ii for ii in range(int(train_data_df.shape[0]))]\n",
    "\n",
    "# Set timestamp columns as index, fillna the training df\n",
    "train_data_df = train_data_df.set_index(\"timestamp\").fillna(0)\n",
    "\n",
    "# Convert the index column to datetime format\n",
    "train_data_df.index = pd.to_datetime(train_data_df.index)\n",
    "train_data_df = train_data_df.select_dtypes(include=['number'])\n",
    "\n",
    "print(\"Training data set shape:\", train_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c9daa52-2cd1-4f68-ab1b-ac5692324ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production data sets (data set name - shape):\n",
      "linear-cpu-stress-ts-station-service-020211 - (181, 12615)\n",
      "linear-cpu-stress-ts-basic-service-020616 - (180, 12195)\n",
      "linear-cpu-stress-ts-train-service-020713 - (181, 12063)\n",
      "linear-memory-stress-ts-station-service-021917 - (241, 15186)\n",
      "linear-memory-stress-ts-train-service-021316 - (240, 14173)\n",
      "linear-memory-stress-ts-basic-service-022009 - (240, 14706)\n",
      "linear-network-delay-ts-station-service-013016 - (180, 12652)\n",
      "linear-network-delay-ts-basic-service-020911 - (181, 13859)\n",
      "linear-network-delay-ts-train-service-020116 - (181, 12449)\n",
      "linear-cpu-train-memory-station-020912 - (181, 14141)\n",
      "linear-cpu-station-memory-train-020810 - (181, 15003)\n",
      "linear-cpu-train-delay-station-021212 - (181, 13827)\n",
      "linear-cpu-station-delay-train-021310 - (180, 12619)\n",
      "linear-memory-train-delay-station-022621 - (240, 16097)\n",
      "linear-memory-station-delay-train-022622 - (241, 15924)\n"
     ]
    }
   ],
   "source": [
    "# --- Load & format production data sets\n",
    "\n",
    "print(\"Production data sets (data set name - shape):\")\n",
    "\n",
    "prod_data_dfs = []\n",
    "for data_set_code in data_sets_to_preprocess:\n",
    "    df_tmp = pd.read_csv(consolidated_data_set_file_path.format(data_set_code=data_set_code)).set_index(\"timestamp\").fillna(0)\n",
    "    df_tmp.index = pd.to_datetime(df_tmp.index)\n",
    "    df_tmp.sort_index(inplace=True)\n",
    "    prod_data_dfs.append(df_tmp)\n",
    "\n",
    "    print(data_set_code, \"-\", df_tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "019c8ac3-3c57-479d-b2ac-6e2e673d33aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear-cpu-stress-ts-station-service-020211 : (19996, 12525) (181, 12525)\n",
      "linear-cpu-stress-ts-basic-service-020616 : (19996, 12525) (180, 12525)\n",
      "linear-cpu-stress-ts-train-service-020713 : (19996, 12525) (181, 12525)\n",
      "linear-memory-stress-ts-station-service-021917 : (19996, 12525) (241, 12525)\n",
      "linear-memory-stress-ts-train-service-021316 : (19996, 12525) (240, 12525)\n",
      "linear-memory-stress-ts-basic-service-022009 : (19996, 12525) (240, 12525)\n",
      "linear-network-delay-ts-station-service-013016 : (19996, 12525) (180, 12525)\n",
      "linear-network-delay-ts-basic-service-020911 : (19996, 12525) (181, 12525)\n",
      "linear-network-delay-ts-train-service-020116 : (19996, 12525) (181, 12525)\n",
      "linear-cpu-train-memory-station-020912 : (19996, 12525) (181, 12525)\n",
      "linear-cpu-station-memory-train-020810 : (19996, 12525) (181, 12525)\n",
      "linear-cpu-train-delay-station-021212 : (19996, 12525) (181, 12525)\n",
      "linear-cpu-station-delay-train-021310 : (19996, 12525) (180, 12525)\n",
      "linear-memory-train-delay-station-022621 : (19996, 12525) (240, 12525)\n",
      "linear-memory-station-delay-train-022622 : (19996, 12525) (241, 12525)\n"
     ]
    }
   ],
   "source": [
    "# --- Align the production datasets' column sets to the train dataset's column set. We use the join by left method\n",
    "\n",
    "if dataset_alignment_method == \"outer_by_training_ds\":\n",
    "    for ii, data_set_code in enumerate(data_sets_to_preprocess):\n",
    "\n",
    "        # - Save the kpis not seen in prod\n",
    "        kpis_not_seen_in_prod_names_transformed = tranform_kpi_names(list(set(train_data_df.columns) - set(prod_data_dfs[ii].columns)), statistics)\n",
    "        # Create a target folder if does not exist\n",
    "        create_dir(kpis_not_seen_in_prod_dir_path)\n",
    "        with open(kpis_not_seen_in_prod_file_path.format(data_set_code=data_set_code), \"w\") as outfile:\n",
    "            csv_writer = csv.writer(outfile)\n",
    "            for row in kpis_not_seen_in_prod_names_transformed:\n",
    "                csv_writer.writerow([row])\n",
    "        \n",
    "        train_data_df, prod_data_dfs[ii] = train_data_df.align(prod_data_dfs[ii], axis=1, join='left')\n",
    "        print(data_set_code, \":\", train_data_df.shape, prod_data_dfs[ii].shape)\n",
    "        \n",
    "\n",
    "if dataset_alignment_method == \"intersection\":\n",
    "    for vv in range(2):\n",
    "        for ii, data_set_code in enumerate(data_sets_to_preprocess):\n",
    "            train_data_df, prod_data_dfs[ii] = train_data_df.align(prod_data_dfs[ii], axis=1, join='inner')\n",
    "            print(data_set_code, \":\", train_data_df.shape, prod_data_dfs[ii].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42a38128-e3b0-4f25-ae91-8d060aeb69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transform the KPI names\n",
    "\n",
    "# Get the tranformed column names\n",
    "transformed_column_names = tranform_kpi_names(train_data_df.columns, statistics)\n",
    "\n",
    "# Update the columns in training data frame\n",
    "train_data_df.columns = transformed_column_names\n",
    "\n",
    "# Update the columns in production data frames\n",
    "for ii in range(len(prod_data_dfs)):\n",
    "    prod_data_dfs[ii].columns = transformed_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a5f35fa-fd50-421f-a343-a400ba934a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean KPI set\n",
    "\n",
    "# KPIs to drop\n",
    "kpis_to_remove = \"distribut|POD_PHASE-failed|POD_PHASE-pending|lm_TotalMedianResponseTime|FAULT-TYPE-minor\"\n",
    "\n",
    "# Drop KPIs from training data frame\n",
    "train_data_df = clean_data(train_data_df, kpis_to_remove)\n",
    "\n",
    "# Drop KPIs from production data frames\n",
    "for ii, data_set_code in enumerate(data_sets_to_preprocess):\n",
    "    prod_data_dfs[ii] = clean_data(prod_data_dfs[ii], kpis_to_remove) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b2a4d2c-de6d-484e-927e-0c2e6d6d45ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop the columns with relatively constant variance True\n",
      "Number of the constant columns: 3007\n"
     ]
    }
   ],
   "source": [
    "# --- Drop the columns with relatively constant variance\n",
    "\n",
    "if DROP_CONSTANT_SERIES:\n",
    "    print(\"Drop the columns with relatively constant variance\", DROP_CONSTANT_SERIES)\n",
    "    \n",
    "    # - Collect the constant KPIs in a list\n",
    "    constant_filter = VarianceThreshold(threshold=0.00001)\n",
    "    constant_filter.fit(train_data_df)\n",
    "\n",
    "    constant_columns = [\n",
    "        column for column in train_data_df.columns\n",
    "        if column not in train_data_df.columns[constant_filter.get_support()] \n",
    "        and column not in columns_to_exclude_from_constant_kpis_search\n",
    "    ]\n",
    "    columns_to_drop = list(set(constant_columns))\n",
    "\n",
    "    print(\"Number of the constant columns:\", len(columns_to_drop))\n",
    "\n",
    "    # Drop the constant columns in training data frame\n",
    "    train_data_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    # Drop the constant columns in production data frames\n",
    "    for ii in range(len(prod_data_dfs)):\n",
    "        prod_data_dfs[ii].drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac3ef1fc-841e-496c-880c-30d49d42fa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of KPIs to apply differencing: 1194\n"
     ]
    }
   ],
   "source": [
    "# --- Differencing: Create a list of KPIs to apply differencing\n",
    "\n",
    "kpi_indexes_to_diff = []\n",
    "\n",
    "# - Get KPIs indexes by the theshold\n",
    "for ii, data_set_code in enumerate(data_sets_to_preprocess):\n",
    "    if data_set_code == \"linear-cpu-station-memory-train-020810\":\n",
    "        kpi_indexes_to_diff = get_cols_to_diff_by_value_threshold(prod_data_dfs[ii], 2)\n",
    "\n",
    "# - Add the KPIs with the specific indexes\n",
    "kpi_indexes_to_diff = set(kpi_indexes_to_diff + [6177, 6178, 6179, 6180, 6181, 6182, 7820, 7794, 7815, 7814, 7811, 7816, 7819, 7793, 7804, 7818, 7789, 7817, 7813, 7812, 7809, 7810, 7811])\n",
    "# TEST kpi_indexes_to_diff = set(kpi_indexes_to_diff + [6177])\n",
    "\n",
    "# - Convert the indexes to KPI names\n",
    "kpi_names_to_diff = [train_data_df.columns[kpi_index] for kpi_index in kpi_indexes_to_diff]\n",
    "\n",
    "# - Add the KPI names by the metric names\n",
    "metrics_to_diff = [115, 167, 112, 94]\n",
    "\n",
    "all_kpis = list(train_data_df.columns)\n",
    "for element in all_kpis:\n",
    "    for metric_to_diff in metrics_to_diff:\n",
    "        metric_to_diff_full_str = \"metric-\" + str(metric_to_diff)\n",
    "        if metric_to_diff_full_str in element:\n",
    "            kpi_names_to_diff.append(element)\n",
    "            break\n",
    "\n",
    "# - Keep only the uniqe KPI names\n",
    "kpis_to_apply_differencing = set(kpi_names_to_diff)\n",
    "\n",
    "print(\"Number of KPIs to apply differencing:\", len(kpis_to_apply_differencing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "411284ca-fa24-46a0-bfbe-c879e1a28e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9t/qlwpj28d5nb9pcvfgv1y9l480000gn/T/ipykernel_50549/4291885313.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  value = dataset[i] - dataset[i - interval]\n"
     ]
    }
   ],
   "source": [
    "# --- Differencing: Apply differency to the training data set\n",
    "\n",
    "if not preprocess_only_prod:\n",
    "    for idx, col in enumerate(train_data_df.columns):\n",
    "        if col in kpis_to_apply_differencing and col != column_name_number_of_users:\n",
    "            if col == \"CONTAINER-NAME-ts-travel-service-FAULT-TYPE_metric-112-min\":\n",
    "                continue\n",
    "\n",
    "            train_data_df[col] = difference_order(train_data_df[col], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71ed3917-40b1-4d67-926d-bdf43b13b4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9t/qlwpj28d5nb9pcvfgv1y9l480000gn/T/ipykernel_50549/4291885313.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  value = dataset[i] - dataset[i - interval]\n"
     ]
    }
   ],
   "source": [
    "# --- Differencing: Apply differencing to the production data sets\n",
    "\n",
    "for data_set_idx, data_set_code in enumerate(data_sets_to_preprocess):\n",
    "    for col_idx, col in enumerate(prod_data_dfs[data_set_idx].columns):\n",
    "        if col in kpis_to_apply_differencing and col != column_name_number_of_users:\n",
    "            if col == \"CONTAINER-NAME-ts-travel-service-FAULT-TYPE_metric-112-min\":\n",
    "                continue\n",
    "\n",
    "            if col in prod_data_dfs[data_set_idx].columns:\n",
    "                prod_data_dfs[data_set_idx][col] = difference_order(prod_data_dfs[data_set_idx][col], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3c821fa-5f39-4b1d-bdf0-eeeb3c9be55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save tunned data: create a folder\n",
    "\n",
    "# Create a target folder if does not exist\n",
    "create_dir(tuned_data_set_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03c11316-e6a5-4dc7-85c5-5a142e60dc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DF saved. Shape: (19996, 8164)\n"
     ]
    }
   ],
   "source": [
    "# --- Save tunned data: training data set\n",
    "\n",
    "if not preprocess_only_prod:\n",
    "    train_data_df.to_csv(tuned_data_set_file_path.format(data_set_code=train_data_set_code), encoding='utf-8', index=False, header=True)\n",
    "    print(\"Train DF saved. Shape:\", train_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0308fc8-8b16-456b-ad77-1611c7b8a8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear-cpu-stress-ts-station-service-020211 DF Saved. Shape:  (181, 8164)\n",
      "linear-cpu-stress-ts-basic-service-020616 DF Saved. Shape:  (180, 8164)\n",
      "linear-cpu-stress-ts-train-service-020713 DF Saved. Shape:  (181, 8164)\n",
      "linear-memory-stress-ts-station-service-021917 DF Saved. Shape:  (241, 8164)\n",
      "linear-memory-stress-ts-train-service-021316 DF Saved. Shape:  (240, 8164)\n",
      "linear-memory-stress-ts-basic-service-022009 DF Saved. Shape:  (240, 8164)\n",
      "linear-network-delay-ts-station-service-013016 DF Saved. Shape:  (180, 8164)\n",
      "linear-network-delay-ts-basic-service-020911 DF Saved. Shape:  (181, 8164)\n",
      "linear-network-delay-ts-train-service-020116 DF Saved. Shape:  (181, 8164)\n",
      "linear-cpu-train-memory-station-020912 DF Saved. Shape:  (181, 8164)\n",
      "linear-cpu-station-memory-train-020810 DF Saved. Shape:  (181, 8164)\n",
      "linear-cpu-train-delay-station-021212 DF Saved. Shape:  (181, 8164)\n",
      "linear-cpu-station-delay-train-021310 DF Saved. Shape:  (180, 8164)\n",
      "linear-memory-train-delay-station-022621 DF Saved. Shape:  (240, 8164)\n",
      "linear-memory-station-delay-train-022622 DF Saved. Shape:  (241, 8164)\n"
     ]
    }
   ],
   "source": [
    "# --- Save tunned data: production data sets\n",
    "\n",
    "for data_set_idx, data_set_code in enumerate(data_sets_to_preprocess):\n",
    "    prod_data_dfs[data_set_idx].to_csv(tuned_data_set_file_path.format(data_set_code=data_set_code), encoding='utf-8', index=False, header=True)\n",
    "    print(\"{data_set_code} DF Saved. Shape: \".format(data_set_code=data_set_code), prod_data_dfs[data_set_idx].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b0bab0c-e2cd-4396-b106-8c9b1ac3a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Tuned Training data\n",
    "\n",
    "if preprocess_only_prod:\n",
    "    train_data_df = pd.read_csv(tuned_data_set_file_path.format(data_set_code=train_data_set_code))\n",
    "    print(train_data_set_code, train_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b5bb3ba-f478-4aea-8a54-5644f6de063d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Build a scaler\n",
    "\n",
    "# Create a numpy.ndarray of the DF's values\n",
    "data_arr = train_data_df.values.astype(float)\n",
    "\n",
    "# Get the train data array\n",
    "train_data, _, _, _ = train_test_split(data_arr, [ii for ii in range(len(data_arr))], test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Build the scaler on the train data array\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd5e18ef-9f6b-4d7f-b0f4-5f8b6dc2fa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19996, 8164)\n"
     ]
    }
   ],
   "source": [
    "# --- Normalize Training data\n",
    "\n",
    "if not preprocess_only_prod:\n",
    "    train_data_df = normalize(scaler, train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c040a65b-271d-4a09-b717-d57590cf131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181, 8164)\n",
      "(180, 8164)\n",
      "(181, 8164)\n",
      "(241, 8164)\n",
      "(240, 8164)\n",
      "(240, 8164)\n",
      "(180, 8164)\n",
      "(181, 8164)\n",
      "(181, 8164)\n",
      "(181, 8164)\n",
      "(181, 8164)\n",
      "(181, 8164)\n",
      "(180, 8164)\n",
      "(240, 8164)\n",
      "(241, 8164)\n"
     ]
    }
   ],
   "source": [
    "# --- Normalize Production data\n",
    "\n",
    "for data_set_idx, data_set_code in enumerate(data_sets_to_preprocess):\n",
    "    prod_data_dfs[data_set_idx] = normalize(scaler, prod_data_dfs[data_set_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d731ce56-d64e-456a-a885-a163ee67b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save normalized data: Create folder\n",
    "\n",
    "# Create a target folder if does not exist\n",
    "create_dir(normalized_data_set_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b03639c1-0698-4d62-a9bc-2f589c531ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DF saved. Shape: (19996, 8164)\n"
     ]
    }
   ],
   "source": [
    "# --- Save normalized data: Training data\n",
    "\n",
    "if not preprocess_only_prod:\n",
    "    train_data_df.to_csv(normalized_data_set_file_path.format(data_set_code=train_data_set_code), encoding='utf-8', index=False, header=True)\n",
    "    print(\"Train DF saved. Shape:\", train_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44a1bc59-16c3-4b2d-ae04-c8dc7346e5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear-cpu-stress-ts-station-service-020211 DF saved. Data shape: (181, 8164)\n",
      "linear-cpu-stress-ts-basic-service-020616 DF saved. Data shape: (180, 8164)\n",
      "linear-cpu-stress-ts-train-service-020713 DF saved. Data shape: (181, 8164)\n",
      "linear-memory-stress-ts-station-service-021917 DF saved. Data shape: (241, 8164)\n",
      "linear-memory-stress-ts-train-service-021316 DF saved. Data shape: (240, 8164)\n",
      "linear-memory-stress-ts-basic-service-022009 DF saved. Data shape: (240, 8164)\n",
      "linear-network-delay-ts-station-service-013016 DF saved. Data shape: (180, 8164)\n",
      "linear-network-delay-ts-basic-service-020911 DF saved. Data shape: (181, 8164)\n",
      "linear-network-delay-ts-train-service-020116 DF saved. Data shape: (181, 8164)\n",
      "linear-cpu-train-memory-station-020912 DF saved. Data shape: (181, 8164)\n",
      "linear-cpu-station-memory-train-020810 DF saved. Data shape: (181, 8164)\n",
      "linear-cpu-train-delay-station-021212 DF saved. Data shape: (181, 8164)\n",
      "linear-cpu-station-delay-train-021310 DF saved. Data shape: (180, 8164)\n",
      "linear-memory-train-delay-station-022621 DF saved. Data shape: (240, 8164)\n",
      "linear-memory-station-delay-train-022622 DF saved. Data shape: (241, 8164)\n"
     ]
    }
   ],
   "source": [
    "# --- Save normalized data: Production data\n",
    "\n",
    "for data_set_idx, data_set_code in enumerate(data_sets_to_preprocess):\n",
    "    prod_data_dfs[data_set_idx].to_csv(normalized_data_set_file_path.format(data_set_code=data_set_code), encoding='utf-8', index=False, header=True)\n",
    "    print(\"{data_set_code} DF saved. Data shape: {data_set_shape}\".format(data_set_code=data_set_code, data_set_shape=prod_data_dfs[data_set_idx].shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernet",
   "language": "python",
   "name": "kernet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
