{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6462e46-d2f2-4014-b6ae-e4e97e98a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from warnings import simplefilter\n",
    "import matplotlib.pyplot as plt\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0479df45-d519-4c99-925b-ffef9402011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Functions\n",
    "\n",
    "%run functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c0a3302-babe-4f78-9cf2-ae36f1f1c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Init Configuration Parameters\n",
    "\n",
    "%run predict_notebook_sections/configuration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60142045-eadd-477c-8667-7b3416dc29b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20074, 4702)\n",
      "(20074, 4702)\n"
     ]
    }
   ],
   "source": [
    "# -- Load Training data\n",
    "\n",
    "train_data_df = pd.read_csv(consolidated_data_set_file_path.format(data_set_code=train_data_set_code)).set_index(\"timestamp\").fillna(0)\n",
    "train_data_df.index = pd.to_datetime(train_data_df.index)\n",
    "print(train_data_df.shape)\n",
    "\n",
    "train_data_df = train_data_df.select_dtypes(include=['number'])\n",
    "print(train_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c9daa52-2cd1-4f68-ab1b-ac5692324ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear-cpu-stress-userapi-051516\n",
      "(181, 4389)\n",
      "linear-memory-stress-userapi-051218\n",
      "(181, 4446)\n",
      "linear-network-loss-userapi-051808\n",
      "(180, 4393)\n",
      "linear-network-delay-userapi-051816\n",
      "(180, 4355)\n",
      "linear-network-corrupt-userapi-050420\n",
      "(189, 4336)\n",
      "linear-network-delay-redis-091409\n",
      "(151, 5786)\n",
      "linear-network-loss-redis-091414\n",
      "(151, 5567)\n",
      "linear-cpu-stress-redis-091514\n",
      "(151, 5808)\n",
      "linear-memory-stress-redis-091522\n",
      "(151, 6015)\n",
      "linear-network-corrupt-redis-091622\n",
      "(151, 5791)\n",
      "linear-network-delay-userhandlers-082208\n",
      "(151, 4165)\n",
      "linear-network-loss-userhandlers-082119\n",
      "(151, 4161)\n",
      "linear-network-corrupt-userhandlers-082114\n",
      "(151, 4363)\n",
      "linear-network-delay-redis-092016\n",
      "(151, 5844)\n",
      "linear-network-loss-redis-092022\n",
      "(151, 5673)\n",
      "linear-network-corrupt-redis-092111\n",
      "(151, 5923)\n",
      "linear-network-delay-scorm-092511\n",
      "(151, 6018)\n",
      "linear-cpu-stress-scorm-092722\n",
      "(151, 5862)\n",
      "linear-memory-stress-scorm-092801\n",
      "(151, 5600)\n"
     ]
    }
   ],
   "source": [
    "# -- Load Prod data\n",
    "\n",
    "prod_data_dfs = []\n",
    "for data_set_code in data_sets_to_preprocess:\n",
    "    print(data_set_code)\n",
    "    \n",
    "    df_tmp = pd.read_csv(consolidated_data_set_file_path.format(data_set_code=data_set_code)).set_index(\"timestamp\").fillna(0)\n",
    "    df_tmp.index = pd.to_datetime(df_tmp.index)\n",
    "    df_tmp.sort_index(inplace=True)\n",
    "    prod_data_dfs.append(df_tmp)\n",
    "    print(df_tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019c8ac3-3c57-479d-b2ac-6e2e673d33aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear-cpu-stress-userapi-051516 : (20074, 4702) (181, 4702)\n",
      "linear-memory-stress-userapi-051218 : (20074, 4702) (181, 4702)\n",
      "linear-network-loss-userapi-051808 : (20074, 4702) (180, 4702)\n",
      "linear-network-delay-userapi-051816 : (20074, 4702) (180, 4702)\n",
      "linear-network-corrupt-userapi-050420 : (20074, 4702) (189, 4702)\n",
      "linear-network-delay-redis-091409 : (20074, 4702) (151, 4702)\n",
      "linear-network-loss-redis-091414 : (20074, 4702) (151, 4702)\n",
      "linear-cpu-stress-redis-091514 : (20074, 4702) (151, 4702)\n",
      "linear-memory-stress-redis-091522 : (20074, 4702) (151, 4702)\n",
      "linear-network-corrupt-redis-091622 : (20074, 4702) (151, 4702)\n",
      "linear-network-delay-userhandlers-082208 : (20074, 4702) (151, 4702)\n",
      "linear-network-loss-userhandlers-082119 : (20074, 4702) (151, 4702)\n",
      "linear-network-corrupt-userhandlers-082114 : (20074, 4702) (151, 4702)\n",
      "linear-network-delay-redis-092016 : (20074, 4702) (151, 4702)\n",
      "linear-network-loss-redis-092022 : (20074, 4702) (151, 4702)\n",
      "linear-network-corrupt-redis-092111 : (20074, 4702) (151, 4702)\n",
      "linear-network-delay-scorm-092511 : (20074, 4702) (151, 4702)\n",
      "linear-cpu-stress-scorm-092722 : (20074, 4702) (151, 4702)\n",
      "linear-memory-stress-scorm-092801 : (20074, 4702) (151, 4702)\n"
     ]
    }
   ],
   "source": [
    "# -- Aligh the prod datasets' column sets to the train dataset's column set\n",
    "# We use outer_by_training_ds\n",
    "\n",
    "if dataset_alignment_method == \"outer_by_training_ds\":\n",
    "    \n",
    "    for ii, data_set_code in enumerate(data_sets_to_preprocess):\n",
    "\n",
    "        # - Save the kpis not seen in prod\n",
    "        \n",
    "        kpis_not_seen_in_prod_names_transformed = tranform_kpi_names(list(set(train_data_df.columns) - set(prod_data_dfs[ii].columns)))\n",
    "        # Create a target folder if does not exist\n",
    "        create_dir(kpis_not_seen_in_prod_dir_path)\n",
    "        with open(kpis_not_seen_in_prod_file_path.format(data_set_code=data_set_code), \"w\") as outfile:\n",
    "            csv_writer = csv.writer(outfile)\n",
    "            for row in kpis_not_seen_in_prod_names_transformed:\n",
    "                csv_writer.writerow([row])\n",
    "        \n",
    "        train_data_df, prod_data_dfs[ii] = train_data_df.align(prod_data_dfs[ii], axis=1, join='left')\n",
    "        print(data_set_code, \":\", train_data_df.shape, prod_data_dfs[ii].shape)\n",
    "        \n",
    "\n",
    "\n",
    "if dataset_alignment_method == \"intersection\":\n",
    "    for vv in range(2):\n",
    "        for ii, data_set_code in enumerate(data_sets_to_preprocess):\n",
    "            train_data_df, prod_data_dfs[ii] = train_data_df.align(prod_data_dfs[ii], axis=1, join='inner')\n",
    "            print(data_set_code, \":\", train_data_df.shape, prod_data_dfs[ii].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42a38128-e3b0-4f25-ae91-8d060aeb69be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unknown-node_lm-UserCount</th>\n",
       "      <th>unknown-node_lm-Requests/s</th>\n",
       "      <th>unknown-node_lm-Failures/s</th>\n",
       "      <th>unknown-node_lm-50%</th>\n",
       "      <th>unknown-node_lm-66%</th>\n",
       "      <th>unknown-node_lm-75%</th>\n",
       "      <th>unknown-node_lm-80%</th>\n",
       "      <th>unknown-node_lm-90%</th>\n",
       "      <th>unknown-node_lm-95%</th>\n",
       "      <th>unknown-node_lm-98%</th>\n",
       "      <th>...</th>\n",
       "      <th>alms-core-rui-dcount_gm-64-count</th>\n",
       "      <th>alms-core-rui-dcount_gm-64-firstquartile</th>\n",
       "      <th>alms-core-rui-dcount_gm-64-thirdquartile</th>\n",
       "      <th>alms-core-rui-dmean_gm-64-min</th>\n",
       "      <th>alms-core-rui-dmean_gm-64-max</th>\n",
       "      <th>alms-core-rui-dmean_gm-64-mean</th>\n",
       "      <th>alms-core-rui-dmean_gm-64-median</th>\n",
       "      <th>alms-core-rui-dmean_gm-64-count</th>\n",
       "      <th>alms-core-rui-dmean_gm-64-firstquartile</th>\n",
       "      <th>alms-core-rui-dmean_gm-64-thirdquartile</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-03-28 12:54:00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-28 12:57:00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-28 12:58:00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-28 12:59:00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-28 13:00:00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.359016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4702 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     unknown-node_lm-UserCount  unknown-node_lm-Requests/s  \\\n",
       "timestamp                                                                    \n",
       "2023-03-28 12:54:00                        1.0                    0.000000   \n",
       "2023-03-28 12:57:00                        1.0                    0.300000   \n",
       "2023-03-28 12:58:00                        1.0                    0.300000   \n",
       "2023-03-28 12:59:00                        1.0                    0.300000   \n",
       "2023-03-28 13:00:00                        1.0                    0.359016   \n",
       "\n",
       "                     unknown-node_lm-Failures/s  unknown-node_lm-50%  \\\n",
       "timestamp                                                              \n",
       "2023-03-28 12:54:00                         0.0                180.0   \n",
       "2023-03-28 12:57:00                         0.0                  0.0   \n",
       "2023-03-28 12:58:00                         0.0                  0.0   \n",
       "2023-03-28 12:59:00                         0.0                  0.0   \n",
       "2023-03-28 13:00:00                         0.0               1700.0   \n",
       "\n",
       "                     unknown-node_lm-66%  unknown-node_lm-75%  \\\n",
       "timestamp                                                       \n",
       "2023-03-28 12:54:00                180.0                180.0   \n",
       "2023-03-28 12:57:00                  0.0                  0.0   \n",
       "2023-03-28 12:58:00                  0.0                  0.0   \n",
       "2023-03-28 12:59:00                  0.0                  0.0   \n",
       "2023-03-28 13:00:00               1700.0               1700.0   \n",
       "\n",
       "                     unknown-node_lm-80%  unknown-node_lm-90%  \\\n",
       "timestamp                                                       \n",
       "2023-03-28 12:54:00                180.0                180.0   \n",
       "2023-03-28 12:57:00                  0.0                  0.0   \n",
       "2023-03-28 12:58:00                  0.0                  0.0   \n",
       "2023-03-28 12:59:00                  0.0                  0.0   \n",
       "2023-03-28 13:00:00               1700.0               1700.0   \n",
       "\n",
       "                     unknown-node_lm-95%  unknown-node_lm-98%  ...  \\\n",
       "timestamp                                                      ...   \n",
       "2023-03-28 12:54:00                180.0                180.0  ...   \n",
       "2023-03-28 12:57:00                  0.0                  0.0  ...   \n",
       "2023-03-28 12:58:00                  0.0                  0.0  ...   \n",
       "2023-03-28 12:59:00                  0.0                  0.0  ...   \n",
       "2023-03-28 13:00:00               1700.0               1700.0  ...   \n",
       "\n",
       "                     alms-core-rui-dcount_gm-64-count  \\\n",
       "timestamp                                               \n",
       "2023-03-28 12:54:00                               0.0   \n",
       "2023-03-28 12:57:00                               0.0   \n",
       "2023-03-28 12:58:00                               0.0   \n",
       "2023-03-28 12:59:00                               0.0   \n",
       "2023-03-28 13:00:00                               0.0   \n",
       "\n",
       "                     alms-core-rui-dcount_gm-64-firstquartile  \\\n",
       "timestamp                                                       \n",
       "2023-03-28 12:54:00                                       0.0   \n",
       "2023-03-28 12:57:00                                       0.0   \n",
       "2023-03-28 12:58:00                                       0.0   \n",
       "2023-03-28 12:59:00                                       0.0   \n",
       "2023-03-28 13:00:00                                       0.0   \n",
       "\n",
       "                     alms-core-rui-dcount_gm-64-thirdquartile  \\\n",
       "timestamp                                                       \n",
       "2023-03-28 12:54:00                                       0.0   \n",
       "2023-03-28 12:57:00                                       0.0   \n",
       "2023-03-28 12:58:00                                       0.0   \n",
       "2023-03-28 12:59:00                                       0.0   \n",
       "2023-03-28 13:00:00                                       0.0   \n",
       "\n",
       "                     alms-core-rui-dmean_gm-64-min  \\\n",
       "timestamp                                            \n",
       "2023-03-28 12:54:00                            0.0   \n",
       "2023-03-28 12:57:00                            0.0   \n",
       "2023-03-28 12:58:00                            0.0   \n",
       "2023-03-28 12:59:00                            0.0   \n",
       "2023-03-28 13:00:00                            0.0   \n",
       "\n",
       "                     alms-core-rui-dmean_gm-64-max  \\\n",
       "timestamp                                            \n",
       "2023-03-28 12:54:00                            0.0   \n",
       "2023-03-28 12:57:00                            0.0   \n",
       "2023-03-28 12:58:00                            0.0   \n",
       "2023-03-28 12:59:00                            0.0   \n",
       "2023-03-28 13:00:00                            0.0   \n",
       "\n",
       "                     alms-core-rui-dmean_gm-64-mean  \\\n",
       "timestamp                                             \n",
       "2023-03-28 12:54:00                             0.0   \n",
       "2023-03-28 12:57:00                             0.0   \n",
       "2023-03-28 12:58:00                             0.0   \n",
       "2023-03-28 12:59:00                             0.0   \n",
       "2023-03-28 13:00:00                             0.0   \n",
       "\n",
       "                     alms-core-rui-dmean_gm-64-median  \\\n",
       "timestamp                                               \n",
       "2023-03-28 12:54:00                               0.0   \n",
       "2023-03-28 12:57:00                               0.0   \n",
       "2023-03-28 12:58:00                               0.0   \n",
       "2023-03-28 12:59:00                               0.0   \n",
       "2023-03-28 13:00:00                               0.0   \n",
       "\n",
       "                     alms-core-rui-dmean_gm-64-count  \\\n",
       "timestamp                                              \n",
       "2023-03-28 12:54:00                              0.0   \n",
       "2023-03-28 12:57:00                              0.0   \n",
       "2023-03-28 12:58:00                              0.0   \n",
       "2023-03-28 12:59:00                              0.0   \n",
       "2023-03-28 13:00:00                              0.0   \n",
       "\n",
       "                     alms-core-rui-dmean_gm-64-firstquartile  \\\n",
       "timestamp                                                      \n",
       "2023-03-28 12:54:00                                      0.0   \n",
       "2023-03-28 12:57:00                                      0.0   \n",
       "2023-03-28 12:58:00                                      0.0   \n",
       "2023-03-28 12:59:00                                      0.0   \n",
       "2023-03-28 13:00:00                                      0.0   \n",
       "\n",
       "                     alms-core-rui-dmean_gm-64-thirdquartile  \n",
       "timestamp                                                     \n",
       "2023-03-28 12:54:00                                      0.0  \n",
       "2023-03-28 12:57:00                                      0.0  \n",
       "2023-03-28 12:58:00                                      0.0  \n",
       "2023-03-28 12:59:00                                      0.0  \n",
       "2023-03-28 13:00:00                                      0.0  \n",
       "\n",
       "[5 rows x 4702 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Transform the KPI names into the {ServiceName}_{MetricName} format\n",
    "# We do it\n",
    "\n",
    "# Get the tranformed column names\n",
    "transformed_column_names = tranform_kpi_names(train_data_df.columns)\n",
    "\n",
    "# Update dfs\n",
    "train_data_df.columns = transformed_column_names\n",
    "for ii in range(len(prod_data_dfs)):\n",
    "    prod_data_dfs[ii].columns = transformed_column_names\n",
    "\n",
    "\n",
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09f4ac5f-df87-43cd-9e95-a4068c4bf131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear-cpu-stress-userapi-051516\n",
      "linear-memory-stress-userapi-051218\n",
      "linear-network-loss-userapi-051808\n",
      "linear-network-delay-userapi-051816\n",
      "linear-network-corrupt-userapi-050420\n",
      "linear-network-delay-redis-091409\n",
      "linear-network-loss-redis-091414\n",
      "linear-cpu-stress-redis-091514\n",
      "linear-memory-stress-redis-091522\n",
      "linear-network-corrupt-redis-091622\n",
      "linear-network-delay-userhandlers-082208\n",
      "linear-network-loss-userhandlers-082119\n",
      "linear-network-corrupt-userhandlers-082114\n",
      "linear-network-delay-redis-092016\n",
      "linear-network-loss-redis-092022\n",
      "linear-network-corrupt-redis-092111\n",
      "linear-network-delay-scorm-092511\n",
      "linear-cpu-stress-scorm-092722\n",
      "linear-memory-stress-scorm-092801\n"
     ]
    }
   ],
   "source": [
    "# -- Create and save the prod data sets configuration from the experminetal log\n",
    "\n",
    "# Load the experminetal log\n",
    "df_experiment_log = pd.read_csv(failure_log)\n",
    "\n",
    "# Parse the experminetal log\n",
    "data_sets_configs = []\n",
    "for data_set_idx, data_set_code in enumerate(data_sets_to_preprocess):\n",
    "\n",
    "    if data_set_code in normal_data_sets:\n",
    "        continue\n",
    "\n",
    "    print(data_set_code)\n",
    "\n",
    "    start_failure_time = round_time(df_experiment_log[df_experiment_log[\"folder_name\"] == data_set_code][\"failure_begin_timestamp\"].iloc[0])\n",
    "    disruption_minute = int(df_experiment_log[df_experiment_log[\"folder_name\"] == data_set_code][\"Disruption\"].iloc[0])\n",
    "    start_failure_minute = np.where(prod_data_dfs[data_set_idx].index == start_failure_time)[0][0]\n",
    "    exp_duration = prod_data_dfs[data_set_idx].shape[0] - 1\n",
    "\n",
    "    the_config = [data_set_code, exp_duration, start_failure_minute, disruption_minute]\n",
    "    data_sets_configs.append(the_config)\n",
    "\n",
    "# Save the prod data sets configuration\n",
    "create_dir(\"/\".join(data_sets_config_file_path.split(\"/\")[:-1]))\n",
    "with open(data_sets_config_file_path, 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow([\"code\", \"total\", \"fi_start\", \"fi_end\"])\n",
    "    write.writerows(data_sets_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c951dbf-2ae0-4719-aca8-e67f425c8272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# -- Exclude from the training data set the rows where the workload is <= user_count_threshold\n",
    "# We do it\n",
    "\n",
    "if DROP_LOW_TRAFFIC_POINTS_FROM_TRAINING:\n",
    "    print(DROP_LOW_TRAFFIC_POINTS_FROM_TRAINING)\n",
    "\n",
    "    train_data_df = train_data_df[train_data_df[column_name_number_of_users] >= DROP_LOW_TRAFFIC_POINTS_THRESHOLD]\n",
    "    for ii in range(len(prod_data_dfs)):\n",
    "        prod_data_dfs[ii] = prod_data_dfs[ii][prod_data_dfs[ii][column_name_number_of_users] >= DROP_LOW_TRAFFIC_POINTS_THRESHOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2a4d2c-de6d-484e-927e-0c2e6d6d45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Drop the constant columns\n",
    "# We do it\n",
    "\n",
    "if DROP_CONSTANT_SERIES:\n",
    "\n",
    "    print(DROP_CONSTANT_SERIES)\n",
    "    \n",
    "    # - Collect the constant KPIs in a list\n",
    "\n",
    "    constant_filter = VarianceThreshold(threshold=0.00001)\n",
    "    constant_filter.fit(train_data_df)\n",
    "\n",
    "    constant_columns = [\n",
    "        column for column in train_data_df.columns\n",
    "        if column not in train_data_df.columns[constant_filter.get_support()] \n",
    "        and column not in columns_to_exclude_from_constant_kpis_search\n",
    "    ]\n",
    "    columns_to_drop = list(set(constant_columns))\n",
    "\n",
    "    print(\"Number of the constant columns:\", len(columns_to_drop))\n",
    "    for col_ in columns_to_drop:\n",
    "        print(col_)\n",
    "\n",
    "    # Drop the constant columns in training data\n",
    "    train_data_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    # Drop the constant columns in prod data\n",
    "    for ii in range(len(prod_data_dfs)):\n",
    "        prod_data_dfs[ii].drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7faf369-5e9e-4f46-9ac9-f6db2f1d3f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Drop the columns that do not contain service name or kube node name\n",
    "# We don't do it\n",
    "\n",
    "if DROP_KPIS_WITH_UNKNOWN_NODES:\n",
    "    print(DROP_KPIS_WITH_UNKNOWN_NODES)\n",
    "    \n",
    "    columns_to_drop = []\n",
    "    for column_name in list(train_data_df.columns):\n",
    "        resource_name = column_name.split(\"_\")[0]\n",
    "        if (resource_name not in LMS_SERVICE_LIST) and (resource_name[:4] not in KUBE_NODE_LIST):\n",
    "            columns_to_drop.append(column_name)\n",
    "            \n",
    "    train_data_df.drop(columns=columns_to_drop, inplace=True)\n",
    "    for ii in range(len(prod_data_dfs)):\n",
    "        prod_data_dfs[ii].drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f7d2b5b-9b13-4e18-a2e3-1b6f1344e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Drop locust metrics\n",
    "# We don't do it\n",
    "\n",
    "if DROP_LOCUST_KPIS:\n",
    "    print(DROP_LOCUST_KPIS)\n",
    "\n",
    "    columns_to_drop = [column_name for column_name in list(train_data_df.columns) if column_name.split(\"_\")[0] == locust_node_name]\n",
    "    \n",
    "    train_data_df.drop(columns=columns_to_drop, inplace=True)\n",
    "    for ii in range(len(prod_data_dfs)):\n",
    "        prod_data_dfs[ii].drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "654952ac-3e6b-4068-a395-2026428c5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Drop the columns of the extended set\n",
    "# We don't do it\n",
    "\n",
    "if DROP_EXTENDED_KPI_SET:\n",
    "    print(DROP_EXTENDED_KPI_SET)\n",
    "    \n",
    "    train_data_df = drop_columns_by_filter(train_data_df, set_of_kpis_to_drop)\n",
    "    for ii in range(len(prod_data_dfs)):\n",
    "        prod_data_dfs[ii] = drop_columns_by_filter(prod_data_dfs[ii], set_of_kpis_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a779aae-e28c-47f3-86b1-e3b064d1450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Apply the first order differencing to all collumns\n",
    "# We don't do it\n",
    "\n",
    "if APPLY_DIFFERENCING:\n",
    "    print(APPLY_DIFFERENCING)\n",
    "    \n",
    "    if kpis_to_apply_differencing == \"all\":\n",
    "        kpis_to_apply_differencing = list(train_data_df.columns)\n",
    "    else:\n",
    "        kpis_to_apply_differencing = kpis_to_apply_differencing.split(\",\")\n",
    "\n",
    "    # For the training data set\n",
    "    for idx, col in enumerate(train_data_df.columns):\n",
    "        if col in kpis_to_apply_differencing and col != column_name_number_of_users:\n",
    "            train_data_df[col] = difference_order(train_data_df[col], 1, 1)\n",
    "\n",
    "    # For the production data sets\n",
    "    for ii in range(len(prod_data_dfs)):\n",
    "        for idx, col in enumerate(prod_data_dfs[ii].columns):\n",
    "            if col in kpis_to_apply_differencing and col != column_name_number_of_users:\n",
    "                prod_data_dfs[ii][col] = difference_order(prod_data_dfs[ii][col], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "248215a5-64e4-4c65-9817-cc15f340419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Keep only the service related KPIs\n",
    "# We don't do it\n",
    "\n",
    "if KEEP_ONLY_SERVICE_RELATED_KPIS:\n",
    "    print(KEEP_ONLY_SERVICE_RELATED_KPIS)\n",
    "\n",
    "    columns_to_drop = [column_name for column_name in list(train_data_df.columns) if column_name.split(\"_\")[0] not in app_service_list]\n",
    "    \n",
    "    train_data_df.drop(columns=columns_to_drop, inplace=True)\n",
    "    for ii in range(len(prod_data_dfs)):\n",
    "        prod_data_dfs[ii].drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c898ad4-0ac6-4dc2-b68b-3edb95d740d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect & Remove up outliers for the predefined set of KPIs from the Training data set\n",
    "# We don't do it\n",
    "\n",
    "if REMOVE_OUTLIERS:\n",
    "    print(REMOVE_OUTLIERS)\n",
    "\n",
    "    for kpi in list(train_data_df.columns):\n",
    "\n",
    "        if \"count\" in kpi:\n",
    "            continue\n",
    "\n",
    "        Q1 = np.percentile(train_data_df[kpi], 25, method='midpoint')\n",
    "        Q3 = np.percentile(train_data_df[kpi], 75, method='midpoint')\n",
    "        Q95 = np.percentile(train_data_df[kpi], 95, method='midpoint')\n",
    "        \n",
    "        # IQR = Q3 - Q1\n",
    "        # upper = Q3 + 1.5 * IQR\n",
    "        # lower = Q1 - 1.5 * IQR\n",
    "        upper = Q95\n",
    "\n",
    "        upper_array = np.where(train_data_df[kpi] >= upper)[0]\n",
    "        # lower_array = np.where(train_data_df[kpi] <= lower)[0]\n",
    "\n",
    "        train_data_df.drop(index=upper_array, inplace=True, errors='ignore')\n",
    "        # train_data_df.drop(index=lower_array, inplace=True, errors='ignore')\n",
    "        \n",
    "    \n",
    "    print(train_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70e651c5-1910-433a-acc0-07b8c01bb041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DF saved (14514, 3948)\n",
      "linear-cpu-stress-userapi-051516 DF Saved. Shape:  (181, 3948)\n",
      "linear-memory-stress-userapi-051218 DF Saved. Shape:  (181, 3948)\n",
      "linear-network-loss-userapi-051808 DF Saved. Shape:  (180, 3948)\n",
      "linear-network-delay-userapi-051816 DF Saved. Shape:  (180, 3948)\n",
      "linear-network-corrupt-userapi-050420 DF Saved. Shape:  (189, 3948)\n",
      "linear-network-delay-redis-091409 DF Saved. Shape:  (151, 3948)\n",
      "linear-network-loss-redis-091414 DF Saved. Shape:  (151, 3948)\n",
      "linear-cpu-stress-redis-091514 DF Saved. Shape:  (151, 3948)\n",
      "linear-memory-stress-redis-091522 DF Saved. Shape:  (151, 3948)\n",
      "linear-network-corrupt-redis-091622 DF Saved. Shape:  (151, 3948)\n",
      "linear-network-delay-userhandlers-082208 DF Saved. Shape:  (151, 3948)\n",
      "linear-network-loss-userhandlers-082119 DF Saved. Shape:  (151, 3948)\n",
      "linear-network-corrupt-userhandlers-082114 DF Saved. Shape:  (151, 3948)\n",
      "linear-network-delay-redis-092016 DF Saved. Shape:  (151, 3948)\n",
      "linear-network-loss-redis-092022 DF Saved. Shape:  (151, 3948)\n",
      "linear-network-corrupt-redis-092111 DF Saved. Shape:  (151, 3948)\n",
      "linear-network-delay-scorm-092511 DF Saved. Shape:  (151, 3948)\n",
      "linear-cpu-stress-scorm-092722 DF Saved. Shape:  (150, 3948)\n",
      "linear-memory-stress-scorm-092801 DF Saved. Shape:  (151, 3948)\n"
     ]
    }
   ],
   "source": [
    "# -- Save data sets\n",
    "\n",
    "# Create a target folder if does not exist\n",
    "create_dir(tuned_data_set_dir_path)\n",
    "\n",
    "# Save the training data set\n",
    "train_data_df.to_csv(tuned_data_set_file_path.format(data_set_code=train_data_set_code), encoding='utf-8', index=False, header=True)\n",
    "print(\"Train DF saved\", train_data_df.shape)\n",
    "\n",
    "# Save the prod data sets\n",
    "for ii in range(len(prod_data_dfs)):\n",
    "    data_set_code = prod_data_set_codes[ii]\n",
    "    df_tmp = prod_data_dfs[ii]\n",
    "    df_tmp.to_csv(tuned_data_set_file_path.format(data_set_code=data_set_code), encoding='utf-8', index=False, header=True)\n",
    "    \n",
    "    print(\"{data_set_code} DF Saved. Shape: \".format(data_set_code=data_set_code), df_tmp.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Prevent-2023-RP",
   "language": "python",
   "name": "prevent-2023-rp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
