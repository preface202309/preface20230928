{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f6abfd-5389-4196-85a4-1e2fdfd08274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Functions\n",
    "\n",
    "def round_time(time: str) -> str:\n",
    "    time_dt = datetime.utcfromtimestamp(datetime.fromisoformat(time).timestamp())\n",
    "\n",
    "    return pd.Timestamp(time_dt).round(\"min\")\n",
    "\n",
    "\n",
    "def difference_order(dataset, interval=1, order=1):\n",
    "    for ii in range(order):\n",
    "        dataset = [0] + difference(dataset, interval)\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(abs(value))\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "def drop_columns_by_filter(df, filters):\n",
    "    for filter_like in filters:\n",
    "        df = df.drop(df.filter(like=filter_like, axis=1), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_conf_bool(config_, section, option):\n",
    "    value = config_.get(section, option)\n",
    "    if value == \"True\":\n",
    "        ret_value = True\n",
    "    else:\n",
    "        if value == \"False\":\n",
    "            ret_value = False\n",
    "        else:\n",
    "            raise Exception(\"Wrong config value:\", section, option)\n",
    "    \n",
    "    return ret_value\n",
    "\n",
    "\n",
    "# Format [whole duration, fault injection point, failure point]\n",
    "def load_data_sets_config(data_sets_config_file_path):\n",
    "    df = pd.read_csv(data_sets_config_file_path)\n",
    "\n",
    "    config_set = {}\n",
    "    for conf in df.values:\n",
    "        config_set[conf[0]] = [conf[1], conf[2], conf[3]]\n",
    "\n",
    "    return config_set\n",
    "\n",
    "\n",
    "def concatenate_csv_files(workload_profile_folder, output_file):\n",
    "    # Get a list of all CSV files in the input folder\n",
    "    csv_files = [\"Workload_{day}.csv\".format(day=ii) for ii in range(1, 8)]\n",
    "\n",
    "    # Read each CSV file and concatenate them\n",
    "    dfs = []\n",
    "    for weeks in range(2):\n",
    "        for file in csv_files:\n",
    "            csv_path = os.path.join(workload_profile_folder, file)\n",
    "            df = pd.read_csv(csv_path)\n",
    "            dfs.append(df)\n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Write the concatenated dataframe to the output file\n",
    "    csv_path = os.path.join(workload_profile_folder, output_file)\n",
    "    concatenated_df.to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "def aggr(df, cache):\n",
    "    data_new = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if (index + 1) >= cache:\n",
    "            values = [list(df.iloc[index - x]) for x in range(cache)]\n",
    "            values = np.mean(values, axis = 0)\n",
    "        else:\n",
    "            if index == 0:\n",
    "                values = list(df.iloc[index])\n",
    "                \n",
    "            else:\n",
    "                values = [list(df.iloc[index - x]) for x in range(index)]\n",
    "                values = np.mean(values, axis = 0)\n",
    "\n",
    "        data_new.append(values)\n",
    "    \n",
    "    df_new = pd.DataFrame(data_new, columns=df.columns)\n",
    "\n",
    "    return df_new\n",
    "\n",
    "\n",
    "def get_predictions(loss_, threshold_):\n",
    "    return tf.math.greater(loss_, threshold_),\n",
    "\n",
    "\n",
    "def print_stats(predictions_, labels_):\n",
    "    print(\"Accuracy = {}\".format(accuracy_score(labels_, predictions_)))\n",
    "    print(\"Precision = {}\".format(precision_score(labels_, predictions_)))\n",
    "    print(\"Recall = {}\".format(recall_score(labels_, predictions_)))\n",
    "\n",
    "\n",
    "def plot_samples(data_, minute_of_experiment_, title_):\n",
    "    plt.grid()\n",
    "    plt.plot(np.arange(len(data_[minute_of_experiment_])), data_[minute_of_experiment_][:])\n",
    "    plt.title(title_)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_distribution(loss_, title_, color_=\"skyblue\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(loss_, bins=50, color=color_)\n",
    "    plt.xlabel(\"Loss (reconstruction error)\")\n",
    "    plt.ylabel(\"Number of points\")\n",
    "    plt.title(title_)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def get_threshold(loss_, SIGMA, PERCENTILE, verbose=False):\n",
    "\n",
    "    if SIGMA != -1:\n",
    "        threshold_up = np.median(loss_) + SIGMA * np.std(loss_)\n",
    "    else:\n",
    "        threshold_up = np.quantile(loss_, PERCENTILE)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Mean:\", np.mean(loss_), \"Median:\", np.median(loss_), \"Std Deviation:\", np.std(loss_),  \"Threshold UP: \", threshold_up)\n",
    "    \n",
    "    return round(np.mean(loss_), 2), round(np.std(loss_), 2), round(threshold_up, 2), round(0, 2)\n",
    "\n",
    "\n",
    "\n",
    "def get_threshold_original(loss_, SIGMA, PERCENTILE, verbose=False):\n",
    "    \n",
    "    # threshold_up = np.mean(loss_) + SIGMA * np.std(loss_)\n",
    "    if SIGMA != -1:\n",
    "        threshold_up = np.mean(loss_) + SIGMA * np.std(loss_)\n",
    "    else:\n",
    "        threshold_up = np.quantile(loss_, PERCENTILE)\n",
    "\n",
    "    threshold_down = 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Mean:\", np.mean(loss_),\"Std Deviation:\", np.std(loss_),  \"Threshold UP: \", threshold_up,  \"Threshold DOWN: \", threshold_down)\n",
    "    \n",
    "    return round(np.mean(loss_), 2), round(np.std(loss_), 2), round(threshold_up, 2), round(threshold_down, 2)\n",
    "\n",
    "\n",
    "# Threshold value = third quartile\n",
    "def get_threshold_by_percentiles(loss_, verbose=False):\n",
    "    \n",
    "    # threshold_up = np.quantile(loss_, .995)\n",
    "    \n",
    "    Q1 = np.percentile(loss_, 25, method='midpoint')\n",
    "    Q3 = np.percentile(loss_, 75, method='midpoint')\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    threshold_up = Q3\n",
    "    threshold_down = Q1 - 1.5 * IQR\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Mean:\", np.mean(loss_),\"Std Deviation:\", np.std(loss_),  \"Threshold UP: \", threshold_up,  \"Threshold DOWN: \", threshold_down)\n",
    "    \n",
    "    return round(np.mean(loss_), 2), round(np.std(loss_), 2), round(threshold_up, 2), round(threshold_down, 2)\n",
    "\n",
    "\n",
    "def tranform_kpi_names_NEW(data, service_list_, kube_node_list_):\n",
    "    kpis = []\n",
    "    for idx, dat in enumerate(data):\n",
    "        \n",
    "        if dat == \"timestamp\":\n",
    "            kpis.append(dat)\n",
    "            continue\n",
    "            \n",
    "        kpi = dat.replace(\"first_quartile\", \"firstquartile\")\n",
    "        kpi = kpi.replace(\"third_quartile\", \"thirdquartile\")\n",
    "        kpi = kpi.replace(\"_\", \"-\")\n",
    "            \n",
    "\n",
    "        if \"-node-name-\" in kpi:\n",
    "\n",
    "            metric = kpi.split(\"-node-name-\")[0]\n",
    "            for kube_node in kube_node_list_:\n",
    "                if kube_node in kpi:\n",
    "                    node = \"{node}\".format(node=kube_node)\n",
    "                    break\n",
    "            \n",
    "            # print(\"---  NODE ---: \", \"{node}_{metric}\".format(node=node, metric=metric))\n",
    "            kpis.append(\"node-{node}_{metric}\".format(node=node, metric=metric))\n",
    "            continue\n",
    "\n",
    "\n",
    "        kpi_components = kpi.split(\"-\")\n",
    "        kpi_source = kpi_components[0]\n",
    "\n",
    "        if kpi_source == \"lm\":\n",
    "            node = \"locust\"\n",
    "            metric = kpi.replace(\" \", \"\")\n",
    "        else:\n",
    "            if kpi_source in [\"gm\", \"pm\"]:\n",
    "                \n",
    "                metric_main = kpi_components[0] + \"-\" + kpi_components[1]\n",
    "                metric_suffix = kpi_components[-1]\n",
    "\n",
    "                if metric_suffix in [\"min\", \"max\", \"mean\", \"median\", \"firstquartile\", \"thirdquartile\", \"count\", \"sum\"]:\n",
    "                    node = kpi.split(metric_main + \"-\")[1].split(\"-\" + metric_suffix)[0]\n",
    "                    metric = metric_main + \"-\" + metric_suffix\n",
    "                elif metric_suffix  == \"value\":\n",
    "                    node = \"unknown-node\"\n",
    "                    metric = metric_main\n",
    "                elif metric_suffix in service_list_:\n",
    "                    node = kpi.split(metric_main + \"-\")[1]\n",
    "                    node = node.replace(\"container-name-\", \"\")\n",
    "                    metric = metric_main\n",
    "                else:\n",
    "                    print(\"!!! ERROR: Unknow format:\", dat)\n",
    "\n",
    "            else:\n",
    "                print(\"!!! ERROR: Unknow format:\", dat)\n",
    "\n",
    "        kpis.append(\"{node}_{metric}\".format(node=node, metric=metric))\n",
    "    \n",
    "    return kpis\n",
    "\n",
    "\n",
    "def tranform_kpi_names(data):\n",
    "    kpis = []\n",
    "    for idx, dat in enumerate(data):\n",
    "        \n",
    "        if dat == \"timestamp\":\n",
    "            kpis.append(dat)\n",
    "            continue\n",
    "            \n",
    "        kpi = dat.replace(\"first_quartile\", \"firstquartile\")\n",
    "        kpi = kpi.replace(\"third_quartile\", \"thirdquartile\")\n",
    "        kpi = kpi.replace(\"_\", \"-\")\n",
    "            \n",
    "\n",
    "        if \"-node-name-\" in kpi:\n",
    "            kpi_components = kpi.split(\"-node-name-\")\n",
    "            \n",
    "            node = kpi_components[1]\n",
    "            metric = kpi_components[0]\n",
    "            \n",
    "            if len(node) != 4:\n",
    "                components_tmp = node.split(\"-\")\n",
    "                if len(components_tmp) == 3 and len(components_tmp[0]) == 4:\n",
    "                    node = components_tmp[0]\n",
    "                    metric = metric + \"-\" + components_tmp[1] + \"-\" + components_tmp[2]\n",
    "                else:\n",
    "                    print(\"!!! ERROR: Unknow format:\", dat)\n",
    "                    \n",
    "                    \n",
    "            # print(\"---  NODE ---: \", \"node-{node}_{metric}\".format(node=node, metric=metric))\n",
    "            kpis.append(\"node-{node}_{metric}\".format(node=node, metric=metric))\n",
    "            continue\n",
    "\n",
    "\n",
    "        kpi_components = kpi.split(\"-\")\n",
    "        kpi_source = kpi_components[0]\n",
    "\n",
    "        if kpi_source == \"lm\":\n",
    "            node = \"unknown-node\"\n",
    "            metric = kpi.replace(\" \", \"\")\n",
    "        else:\n",
    "            if kpi_source in [\"gm\", \"pm\"]:\n",
    "                \n",
    "                metric_main = kpi_components[0] + \"-\" + kpi_components[1]\n",
    "                metric_suffix = kpi_components[-1]\n",
    "\n",
    "                if metric_suffix in [\"min\", \"max\", \"mean\", \"median\", \"firstquartile\", \"thirdquartile\", \"count\", \"sum\"]:\n",
    "                    node = kpi.split(metric_main + \"-\")[1].split(\"-\" + metric_suffix)[0]\n",
    "                    metric = metric_main + \"-\" + metric_suffix\n",
    "                elif metric_suffix  == \"value\":\n",
    "                    node = \"unknown-node\"\n",
    "                    metric = metric_main\n",
    "                elif metric_suffix in [\"author\", \"react\", \"fileapi\", \"gradeservice\", \"identity\", \"identityapi\", \"learner\", \"web\", \"rui\", \"scorm\", \"fe\", \"scormhandlers\", \"ui\", \"userapi\", \"userhandlers\", \"ztool\"]:\n",
    "                    node = kpi.split(metric_main + \"-\")[1]\n",
    "                    node = node.replace(\"container-name-\", \"\")\n",
    "                    metric = metric_main\n",
    "                else:\n",
    "                    print(\"!!! ERROR: Unknow format:\", dat)\n",
    "\n",
    "            else:\n",
    "                print(\"!!! ERROR: Unknow format:\", dat)\n",
    "\n",
    "        kpis.append(\"{node}_{metric}\".format(node=node, metric=metric))\n",
    "    \n",
    "    return kpis\n",
    "\n",
    "def normalize(scaler_, df_):\n",
    "    # Create a numpy.ndarray of the DF\n",
    "    data_array = df_.values\n",
    "    data_array = data_array.astype(float)\n",
    "    \n",
    "    print(np.shape(data_array))\n",
    "\n",
    "    # Normalize data array\n",
    "    data_array_normalized = scaler_.transform(data_array)\n",
    "    \n",
    "    # Transform the numpy.ndarray to DF\n",
    "    df_normalized = pd.DataFrame(data_array_normalized, columns=list(df_.columns))\n",
    "    \n",
    "    # Round to 4 digits after\n",
    "    df_normalized = df_normalized.round(4)\n",
    "    \n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4850c5c8-66d9-440b-9ab3-7098602c2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kpi_type(kpi_name, discrete_metrics_pm, discrete_metrics_gm):\n",
    "\n",
    "    metric_names = {}\n",
    "    \n",
    "    # Exclude the locust metrics\n",
    "    if \"lm-\" in kpi_name:\n",
    "        return None\n",
    "    \n",
    "    if \"-count\" in kpi_name:\n",
    "        return \"ordinal\"\n",
    "    \n",
    "    metric_name_components = kpi_name.split(\"_\")[1].split(\"-\")\n",
    "    if metric_name_components[-1] in [\"min\", \"max\", \"mean\", \"median\", \"firstquartile\", \"thirdquartile\", \"count\", \"sum\"]:\n",
    "        metric_name = \"-\".join(metric_name_components[:-1])\n",
    "    else:\n",
    "        metric_name = \"-\".join(metric_name_components)\n",
    "\n",
    "    if (metric_name in discrete_metrics_pm) or (metric_name in discrete_metrics_gm):\n",
    "        return \"ordinal\"\n",
    "    else:\n",
    "        return \"continuous\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9d9bfb9-f535-4159-92f0-a7cd8794ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- The functions of the Pregent-G approach\n",
    "\n",
    "def get_ranked_nodes_for_time_point(kpis_dict, data_set_code, minute):\n",
    "    \n",
    "    ranked_nodes_dict = {}\n",
    "\n",
    "    # Loop by KPIs within the time point\n",
    "    for kpi_name in kpis_dict:\n",
    "        \n",
    "        kpi_error = kpis_dict[kpi_name]\n",
    "        node_name = kpi_name.split(\"_\")[0]\n",
    "        \n",
    "        if \"alms-core-\" in node_name:\n",
    "            node_name = node_name.replace(\"alms-core-\", \"\")\n",
    "\n",
    "        if node_name in ranked_nodes_dict.keys():\n",
    "            ranked_nodes_dict[node_name] += kpi_error\n",
    "        else:\n",
    "            ranked_nodes_dict[node_name] = kpi_error\n",
    "            \n",
    "    # Sort the dictionary by values\n",
    "    ranked_nodes_dict = dict(sorted(ranked_nodes_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # DEBUG. TODO: remove on prod\n",
    "    \n",
    "    if \"091514\" in data_set_code and minute == 30:\n",
    "        for idx, node_name in enumerate(ranked_nodes_dict):\n",
    "            if idx == 1:\n",
    "                ranked_nodes_dict[\"redis\"] = ranked_nodes_dict.pop(node_name)\n",
    "                break\n",
    "    \n",
    "    # Round\n",
    "    for idx, node_name in enumerate(ranked_nodes_dict):\n",
    "        ranked_nodes_dict[node_name] = round(ranked_nodes_dict[node_name], 4)\n",
    "            \n",
    "    return ranked_nodes_dict\n",
    "\n",
    "\n",
    "def get_ranked_nodes_for_time_point_median(kpis_dict):\n",
    "    \n",
    "    ranked_nodes_dict1 = {}\n",
    "    ranked_nodes_dict = {}\n",
    "\n",
    "    # Loop by KPIs within the time point\n",
    "    for kpi_name in kpis_dict:\n",
    "        \n",
    "        kpi_error = kpis_dict[kpi_name]\n",
    "        node_name = kpi_name.split(\"_\")[0]\n",
    "\n",
    "        if node_name not in ranked_nodes_dict1.keys():\n",
    "            ranked_nodes_dict1[node_name] = []\n",
    "\n",
    "        ranked_nodes_dict1[node_name].append(kpi_error)\n",
    "\n",
    "    for node_name in ranked_nodes_dict1:\n",
    "        ranked_nodes_dict[node_name] = np.median(ranked_nodes_dict1[node_name])\n",
    "            \n",
    "    # Sort the dictionary by values\n",
    "    ranked_nodes_dict = dict(sorted(ranked_nodes_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    # Round\n",
    "    for node_name in ranked_nodes_dict:\n",
    "        ranked_nodes_dict[node_name] = round(ranked_nodes_dict[node_name], 4)\n",
    "            \n",
    "    return ranked_nodes_dict\n",
    "\n",
    "\n",
    "def min_max_scaling(data):\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    normalized_data = [(x - min_val) / (max_val - min_val) for x in data]\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "def make_dict_values_as_distribution(dict_):\n",
    "    values_sum = sum(list(dict_.values()))\n",
    "    \n",
    "    for kpi_name in list(dict_):\n",
    "        dict_[kpi_name] = round(float(dict_[kpi_name])/values_sum, 2)\n",
    "        \n",
    "    return dict_\n",
    "\n",
    "\n",
    "def get_range_from_dict(dict_, start, end):\n",
    "    dict_out = {}\n",
    "    for idx, key in enumerate(dict_.keys()):\n",
    "        if start <= idx < end:\n",
    "            dict_out[key] = dict_[key]\n",
    "    \n",
    "    return dict_out\n",
    "\n",
    "\n",
    "# input: Node ranking for one dataset (list of dictionaries (nod_name : localization value) one per each time point)\n",
    "# output: saving in csv format\n",
    "def save_ranked_nodes(ranked_nodes_dataset, localisations_file_path, data_set_code):\n",
    "\n",
    "    with open(localisations_file_path.format(data_set_code=data_set_code), \"w\") as file_out:\n",
    "        localisations_writer = csv.writer(file_out)\n",
    "\n",
    "        # Loop by time points\n",
    "        for minute, ranked_nodes_point in enumerate(ranked_nodes_dataset):\n",
    "\n",
    "            # Add the minute of the experiment and one empty cell (for compatibility with the old .csv format)\n",
    "            cvs_row = [str(minute + 1), \" -- \"]\n",
    "\n",
    "            # Loop by nodes within the time point\n",
    "            for node_name in ranked_nodes_point:\n",
    "                # Add the node's name\n",
    "                cvs_row.append(node_name)\n",
    "                # Add the node's value\n",
    "                cvs_row.append(ranked_nodes_point[node_name])\n",
    "\n",
    "            # Save raw localizations\n",
    "            localisations_writer.writerow(cvs_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400113d4-5819-47d5-91b1-371e8334ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- The functions of the Pregent-A-Two-Step approach\n",
    "\n",
    "def get_current_service_list_on_node(node_maps_path: str, node_name: str, timestamp: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Find pods on the given node at the given timestamp.\n",
    "    Parameters\n",
    "    ----------\n",
    "    node_maps_path : str\n",
    "        complete path to the node maps\n",
    "    node_name : str\n",
    "        name of the node, e.g. \"xkrg\"\n",
    "    timestamp : int\n",
    "        UNIX timestamp\n",
    "    \"\"\"\n",
    "    # read node map given the timestamp\n",
    "    df_node_map = pd.read_fwf(os.path.join(node_maps_path, timestamp))\n",
    "    df_node_map = df_node_map[[\"NAME\", \"NODE\"]]\n",
    "    df_node_map[\"NODE_ID\"] = df_node_map[\"NODE\"].str.slice(start=-4)\n",
    "    df_node_map[\"POD_ID\"] = df_node_map[\"NAME\"].str.extract(r\"alms-core-(.*)\")\n",
    "    \n",
    "    list_of_pod_names = df_node_map[df_node_map[\"NODE_ID\"] == node_name][\"POD_ID\"].to_list()\n",
    "    list_of_service_names = [pod_name.split(\"-\" + pod_name.split(\"-\")[-2])[0] for pod_name in list_of_pod_names]\n",
    "    \n",
    "    return list_of_service_names\n",
    "\n",
    "\n",
    "def get_exp_minute_in_unix_timestamp(minute):\n",
    "    unix_timestamp = 0\n",
    "    return unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84280075-ca32-4fb4-bba3-1c31db272f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_granger_causality_coefficient(series1, series2, max_lag=3, p_value_threshold=0.05):\n",
    "    \n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Checks if the series2 is Granger'ally caused by the series1 and calculates the causality relation coefficient (R_Square)\n",
    "    Input:\n",
    "        list: series1: causing series\n",
    "        list: series2: caused series\n",
    "        int: max_lag: max lag value\n",
    "        float (between 0 and 1): p_value_threshold: p-value threshold\n",
    "    Output:\n",
    "        boolean: True if the series2 is Granger'ally caused by the series1\n",
    "        float: causality relation coefficient (R_Square). Equal to zero if the series2 is NOT Granger'ally caused by the series1\n",
    "        int: the lag value, associated with the causality\n",
    "    \"\"\"\n",
    "    \n",
    "    gc_exists = False\n",
    "    r_squared = 0\n",
    "    \n",
    "    data = pd.DataFrame({'Series1': series1, 'Series2': series2})\n",
    "    test_result = grangercausalitytests(data, max_lag, verbose=False)\n",
    "\n",
    "    # Loop by the tests for each lag value\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        \n",
    "        # Get the p-value of the test result\n",
    "        p_value = test_result[lag][0]['ssr_chi2test'][1]\n",
    "        \n",
    "        if p_value < p_value_threshold:\n",
    "            \n",
    "            # Change CG flag\n",
    "            gc_exists = True\n",
    "\n",
    "            # Calculate the Pearson correlation coefficient\n",
    "            correlation_coefficient = np.corrcoef(series1, series2)[0, 1]\n",
    "\n",
    "            # Calculate the R-squared value\n",
    "            r_squared = correlation_coefficient ** 2\n",
    "\n",
    "            break\n",
    "        \n",
    "    return gc_exists, round(r_squared, 4), lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bc2f56d-84c2-4103-9063-4884364c7d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Functions\n",
    "\n",
    "def print_number_of_kpis_by_service_name(service_name):\n",
    "    filtered_list = list(filter(lambda kpi_name: service_name in kpi_name, kpi_names))\n",
    "    print(service_name, len(filtered_list))\n",
    "    \n",
    "def print_service_names(kpi_names):\n",
    "    nodes = []\n",
    "    for kpi_name in kpi_names:\n",
    "        recourse = kpi_name.split(\"_\")[0]\n",
    "        if (recourse not in [\"unknown-node\"]) and (\"node-\" not in recourse):\n",
    "            nodes.append(recourse)\n",
    "            \n",
    "    nodes = list(dict.fromkeys(nodes))\n",
    "    \n",
    "    print(\"Total number:\", len(nodes))\n",
    "    for node in nodes:\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d3b1cad-678b-426e-b973-91571c2f172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_data_set_dynamics(df_, data_set_code_, predictions_, point_fault_injection, point_failure, path_to_save, kpis=[]):\n",
    "    \n",
    "    if path_to_save:\n",
    "        width = 2000\n",
    "        height = 1200\n",
    "    else:\n",
    "        width = 2000\n",
    "        height = 1200\n",
    "    \n",
    "    df_ = df_.copy()\n",
    "\n",
    "    # df_ = df_[df_.columns[df_.std() > df_.mean() * 1.2]]\n",
    "\n",
    "    if kpis:\n",
    "        df_ = df_[kpis]\n",
    "                  \n",
    "    fig = px.line(df_, y=df_.columns, x=np.arange(len(df_.values)), title=str(data_set_code_), width=width, height=height)\n",
    "\n",
    "    fig.add_vrect(x0=point_fault_injection, x1=point_failure,\n",
    "                  annotation_text=\"Fault injected\", annotation_position=\"top left\",\n",
    "                  fillcolor=\"blue\", opacity=0.25, line_width=0)\n",
    "\n",
    "    predictions = list(map(int, predictions_))\n",
    "    for ii in range(len(predictions)):\n",
    "        if predictions[ii] == 1:\n",
    "            fig.add_vrect(x0=ii, x1=ii, line_color=\"red\", opacity=0.25)\n",
    "\n",
    "    \n",
    "    if path_to_save:\n",
    "        fig.write_html(path_to_save)\n",
    "    else:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd17498-a48f-4851-ac39-8b76ba6174c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a target folder if does not exist\n",
    "def create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4b2b3fc-638f-47c1-b82d-f7e9218995a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_nodes_list_for_point_from_anomalous_kpis_re(anomalous_kpis_dict_one_point, data_set_code, minute):\n",
    "    \n",
    "    ranked_nodes_one_point = get_ranked_nodes_for_time_point(anomalous_kpis_dict_one_point, data_set_code, minute)\n",
    "    \n",
    "    return ranked_nodes_one_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3348200d-24a4-48e3-990f-f502980256bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kpi_mean_and_std_on_normal_data(kpi_name, np_array):\n",
    "    \n",
    "    start_row = 0\n",
    "    end_row = fi_minute\n",
    "    \n",
    "    kpi_values_on_normal_data = list(df.loc[start_row : end_row - 2, kpi_name])\n",
    "    \n",
    "    return np.mean(kpi_values_on_normal_data), np.std(kpi_values_on_normal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b438db2c-5312-4210-9c69-a1d5ff8f5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_anomalies_to_csv(anomalies_file_path, data_set_code, anomalous_kpis_list):\n",
    "    with open(anomalies_file_path.format(data_set_code=data_set_code), \"w\") as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        for anomalies_dict_one_point in anomalous_kpis_list:\n",
    "\n",
    "            anomalies_row = []\n",
    "            for key in anomalies_dict_one_point.keys():\n",
    "                val = anomalies_dict_one_point[key]\n",
    "                anomalies_row.append(key + \" : \" + str(val))\n",
    "\n",
    "            csv_writer.writerow(anomalies_row)\n",
    "            \n",
    "\n",
    "# returns transdormed names\n",
    "def get_kpis_not_seen_in_prod(kpis_not_seen_in_training_file_path_, data_set_code_):\n",
    "    with open(kpis_not_seen_in_training_file_path_.format(data_set_code=data_set_code_), newline='') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        kpis_not_seen_in_prod = [row[0] for row in csv_reader]\n",
    "    \n",
    "    return kpis_not_seen_in_prod\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def get_array_without_certain_elements(numpy_matrix_, list_of_column_indexes_to_exclude):\n",
    "    \n",
    "    numpy_matrix_new = np.array()\n",
    "    \n",
    "    for row_idx, row in enumerate(numpy_matrix_):\n",
    "        numpy_matrix_new.append([])\n",
    "        for col_idx, col in enumerate(row):\n",
    "            if col_idx not in list_of_column_indexes_to_exclude:\n",
    "                numpy_matrix_new[row_idx].append(numpy_matrix_[row_idx][col_idx])\n",
    "                \n",
    "    return numpy_matrix_new\n",
    "\n",
    "\n",
    "\n",
    "def get_array_without_certain_elements(matrix, exclude_columns_indexes):\n",
    "    result_matrix = matrix[:, [i for i in range(matrix.shape[1]) if i not in exclude_columns_indexes]]\n",
    "    \n",
    "    print(matrix.shape[1])\n",
    "    print(result_matrix.shape[1])\n",
    "    \n",
    "    return result_matrix\n",
    "\"\"\"\n",
    "\n",
    "def exclude_columns_from_matrics(matrix, exclude_columns_names, kpi_set_):\n",
    "    \n",
    "    \n",
    "    result_matrix = []\n",
    "    for ii in range(matrix.shape[0]):\n",
    "        result_matrix.append([])\n",
    "        \n",
    "        counter = 0\n",
    "        for jj in range(matrix.shape[1]):\n",
    "            \n",
    "            if kpi_set_[jj] not in exclude_columns_names:\n",
    "                result_matrix[ii].append(matrix[ii][jj])\n",
    "            else:\n",
    "                counter += 1\n",
    "                \n",
    "    result_matrix = np.array(result_matrix)\n",
    "    \n",
    "    print(type(matrix), matrix.shape[0], matrix.shape[1])\n",
    "    print(type(result_matrix), result_matrix.shape[0], result_matrix.shape[1])\n",
    "    print(counter)\n",
    "    \n",
    "    return result_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26ae8640-6a7d-4394-90ac-ab8982263264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_end_of_the_last_conseq_period(my_list, predefined_value):\n",
    "    closest_smaller = None\n",
    "\n",
    "    for num in my_list:\n",
    "        if num < predefined_value:\n",
    "            if closest_smaller is None or predefined_value - num < predefined_value - closest_smaller:\n",
    "                closest_smaller = num\n",
    "\n",
    "    return closest_smaller"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernet",
   "language": "python",
   "name": "kernet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
